{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11391d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 – imports\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRanker, Pool\n",
    "\n",
    "META_PATH = '../data/outputs/dataset_meta.json'\n",
    "TRAIN_RANK_PATH = '../data/outputs/train_rank.parquet'\n",
    "VALID_RANK_PATH = '../data/outputs/valid_rank.parquet'\n",
    "GROUP_VALID_PATH = '../data/outputs/groups_valid.npy'\n",
    "FEATURES_PATH = '../data/outputs/features_week=20200922.parquet'\n",
    "SAMPLE_PATH = '../data/input_data/sample_submission.csv'\n",
    "GENERAL_PATH = '../data/outputs/general_pred_str.json'\n",
    "MODEL_PATH = '../data/outputs/catboost_ranker.model'\n",
    "SUB_PATH = '../data/submission/catboost_ranker_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8971e143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 8180082\n",
      "Valid rows: 253714158\n",
      "Features: 34\n",
      "Groups train: 294983 valid: 1371980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2 – load train/valid ranking data\n",
    "META_PATH = '../data/outputs/dataset_meta.json'\n",
    "TRAIN_RANK_PATH = '../data/outputs/train_rank.parquet'\n",
    "VALID_RANK_PATH = '../data/outputs/valid_rank.parquet'\n",
    "GROUP_TRAIN_PATH = '../data/outputs/groups_train.npy'\n",
    "GROUP_VALID_PATH = '../data/outputs/groups_valid.npy'\n",
    "\n",
    "with open(META_PATH) as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "feature_cols = meta['model_features']\n",
    "\n",
    "# load\n",
    "train_df = pd.read_parquet(TRAIN_RANK_PATH)\n",
    "valid_df = pd.read_parquet(VALID_RANK_PATH)\n",
    "\n",
    "# REMOVE FEATURES\n",
    "remove_features = [\n",
    "    # 'value',\n",
    "    # 'age_bucket',\n",
    "    # 'customer_total_purchases',\n",
    "    # 'customer_unique_articles',\n",
    "    # 'cust_purchases_1w',\n",
    "    # 'cust_purchases_4w',\n",
    "    # 'article_total_purchases',\n",
    "    # 'article_unique_customers',\n",
    "    #'club_member_status',\n",
    "    #'fashion_news_frequency',\n",
    "    #'age',\n",
    "    #'postal_code',\n",
    "    #'product_code',\n",
    "    #'product_type_no',\n",
    "    #'product_group_name',\n",
    "    #'graphical_appearance_no',\n",
    "    #'colour_group_code',\n",
    "    #'perceived_colour_value_id',\n",
    "    #'perceived_colour_master_id',\n",
    "    #'department_no',\n",
    "    #'index_code',\n",
    "    #'index_group_no',\n",
    "    #'section_no',\n",
    "    #'garment_group_no',\n",
    "    # 'article_mean_price',\n",
    "    # 'customer_mean_price',\n",
    "    # 'article_mean_age',\n",
    "    # 'index_group_match',\n",
    "    # 'product_code_match',\n",
    "    # 'days_since_last_purchase',\n",
    "    # 'customer_days_since_last_purchase',\n",
    "    # 'price_sensitivity',\n",
    "    # 'age_sensitivity',\n",
    "    # 'window_type_code',\n",
    "]\n",
    "\n",
    "# drop removed features\n",
    "drop_cols = [c for c in remove_features if c in train_df.columns]\n",
    "if drop_cols:\n",
    "    print(\"Dropping:\", drop_cols)\n",
    "    train_df = train_df.drop(columns=drop_cols)\n",
    "    valid_df = valid_df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# update feature_cols\n",
    "feature_cols = [c for c in feature_cols if c not in remove_features]\n",
    "\n",
    "train_group = np.load(GROUP_TRAIN_PATH)\n",
    "valid_group = np.load(GROUP_VALID_PATH)\n",
    "\n",
    "# float32 for GPU efficiency\n",
    "for c in feature_cols:\n",
    "    if pd.api.types.is_float_dtype(train_df[c]):\n",
    "        train_df[c] = train_df[c].astype('float32')\n",
    "        valid_df[c] = valid_df[c].astype('float32')\n",
    "\n",
    "train_df['label'] = train_df['label'].astype('float32')\n",
    "valid_df['label'] = valid_df['label'].astype('float32')\n",
    "\n",
    "print(\"Train rows:\", len(train_df))\n",
    "print(\"Valid rows:\", len(valid_df))\n",
    "print(\"Features:\", len(feature_cols))\n",
    "print(\"Groups train:\", len(train_group), \"valid:\", len(valid_group))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ce40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering validation set to groups with at least 1 positive...\n",
      "Filtered valid groups: 27802\n",
      "Filtered valid rows: 8372080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3 – filter validation groups with ≥1 positive\n",
    "\n",
    "print(\"\\nFiltering validation set to groups with at least 1 positive...\")\n",
    "\n",
    "orig_valid_group = np.load(GROUP_VALID_PATH)\n",
    "group_bounds = np.insert(np.cumsum(orig_valid_group), 0, 0)[:-1]\n",
    "group_has_positive = np.add.reduceat(valid_df['label'].values, group_bounds) > 0\n",
    "\n",
    "valid_group = orig_valid_group[group_has_positive]\n",
    "row_mask = np.repeat(group_has_positive, orig_valid_group)\n",
    "valid_df = valid_df[row_mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Filtered valid groups:\", len(valid_group))\n",
    "print(\"Filtered valid rows:\", len(valid_df))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aaad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded train_group_id: 8180082\n",
      "Expanded valid_group_id: 8372080\n",
      "Train_df rows: 8180082\n",
      "Valid_df rows: 8372080\n",
      "Pools built:\n",
      "Train rows: 8180082\n",
      "Valid rows: 8372080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4 – Build CatBoost Pools\n",
    "\n",
    "from catboost import Pool\n",
    "\n",
    "# Convert group-sizes to per-row group_id (CatBoost requirement)\n",
    "def expand_group_ids(group_sizes):\n",
    "    ids = np.repeat(np.arange(len(group_sizes), dtype=np.int32), group_sizes)\n",
    "    return ids\n",
    "\n",
    "train_group_id = expand_group_ids(train_group)\n",
    "valid_group_id = expand_group_ids(valid_group)\n",
    "\n",
    "print(\"Expanded train_group_id:\", len(train_group_id))\n",
    "print(\"Expanded valid_group_id:\", len(valid_group_id))\n",
    "print(\"Train_df rows:\", len(train_df))\n",
    "print(\"Valid_df rows:\", len(valid_df))\n",
    "\n",
    "assert len(train_group_id) == len(train_df)\n",
    "assert len(valid_group_id) == len(valid_df)\n",
    "\n",
    "# Treat all features as numeric\n",
    "cat_features = []\n",
    "\n",
    "pool_train = Pool(\n",
    "    data=train_df[feature_cols],\n",
    "    label=train_df[\"label\"],\n",
    "    group_id=train_group_id,\n",
    "    cat_features=cat_features\n",
    ")\n",
    "\n",
    "pool_valid = Pool(\n",
    "    data=valid_df[feature_cols],\n",
    "    label=valid_df[\"label\"],\n",
    "    group_id=valid_group_id,\n",
    "    cat_features=cat_features\n",
    ")\n",
    "\n",
    "print(\"Pools built:\")\n",
    "print(\"Train rows:\", pool_train.shape[0])\n",
    "print(\"Valid rows:\", pool_valid.shape[0])\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5bd5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because PFound, NDCG is/are not implemented for GPU\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
      "Metric NDCG:top=12;type=Base is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.1132206\tbest: 0.1132206 (0)\ttotal: 452ms\tremaining: 1h 15m 20s\n",
      "50:\ttest: 0.1508516\tbest: 0.1511016 (49)\ttotal: 9.07s\tremaining: 29m 30s\n",
      "100:\ttest: 0.1564645\tbest: 0.1565230 (99)\ttotal: 17.9s\tremaining: 29m 12s\n",
      "150:\ttest: 0.1597213\tbest: 0.1597213 (150)\ttotal: 26.8s\tremaining: 29m 8s\n",
      "200:\ttest: 0.1614503\tbest: 0.1614503 (200)\ttotal: 35.7s\tremaining: 29m\n",
      "250:\ttest: 0.1629685\tbest: 0.1629685 (250)\ttotal: 44.6s\tremaining: 28m 50s\n",
      "300:\ttest: 0.1639570\tbest: 0.1639570 (300)\ttotal: 53.5s\tremaining: 28m 45s\n",
      "350:\ttest: 0.1647285\tbest: 0.1647633 (349)\ttotal: 1m 2s\tremaining: 28m 32s\n",
      "400:\ttest: 0.1652624\tbest: 0.1654592 (395)\ttotal: 1m 11s\tremaining: 28m 21s\n",
      "450:\ttest: 0.1656133\tbest: 0.1656133 (450)\ttotal: 1m 19s\tremaining: 28m 7s\n",
      "500:\ttest: 0.1659550\tbest: 0.1660813 (493)\ttotal: 1m 28s\tremaining: 27m 54s\n",
      "550:\ttest: 0.1662942\tbest: 0.1663778 (547)\ttotal: 1m 37s\tremaining: 27m 44s\n",
      "600:\ttest: 0.1665069\tbest: 0.1666384 (593)\ttotal: 1m 45s\tremaining: 27m 35s\n",
      "650:\ttest: 0.1667524\tbest: 0.1668090 (640)\ttotal: 1m 54s\tremaining: 27m 25s\n",
      "700:\ttest: 0.1668392\tbest: 0.1668717 (699)\ttotal: 2m 3s\tremaining: 27m 17s\n",
      "750:\ttest: 0.1669137\tbest: 0.1669382 (737)\ttotal: 2m 12s\tremaining: 27m 7s\n",
      "800:\ttest: 0.1672051\tbest: 0.1672409 (799)\ttotal: 2m 20s\tremaining: 26m 59s\n",
      "850:\ttest: 0.1673614\tbest: 0.1674388 (822)\ttotal: 2m 29s\tremaining: 26m 50s\n",
      "900:\ttest: 0.1673051\tbest: 0.1674542 (895)\ttotal: 2m 38s\tremaining: 26m 41s\n",
      "950:\ttest: 0.1675266\tbest: 0.1675478 (947)\ttotal: 2m 47s\tremaining: 26m 32s\n",
      "1000:\ttest: 0.1677423\tbest: 0.1677915 (996)\ttotal: 2m 56s\tremaining: 26m 23s\n",
      "1050:\ttest: 0.1679635\tbest: 0.1680259 (1045)\ttotal: 3m 4s\tremaining: 26m 15s\n",
      "1100:\ttest: 0.1679967\tbest: 0.1680800 (1053)\ttotal: 3m 13s\tremaining: 26m 5s\n",
      "1150:\ttest: 0.1682413\tbest: 0.1682413 (1150)\ttotal: 3m 22s\tremaining: 25m 56s\n",
      "1200:\ttest: 0.1683884\tbest: 0.1683884 (1200)\ttotal: 3m 31s\tremaining: 25m 47s\n",
      "1250:\ttest: 0.1684304\tbest: 0.1685513 (1221)\ttotal: 3m 40s\tremaining: 25m 39s\n",
      "1300:\ttest: 0.1684495\tbest: 0.1686964 (1280)\ttotal: 3m 48s\tremaining: 25m 30s\n",
      "1350:\ttest: 0.1687147\tbest: 0.1687205 (1320)\ttotal: 3m 57s\tremaining: 25m 21s\n",
      "1400:\ttest: 0.1687994\tbest: 0.1688714 (1388)\ttotal: 4m 6s\tremaining: 25m 12s\n",
      "1450:\ttest: 0.1689752\tbest: 0.1690568 (1429)\ttotal: 4m 15s\tremaining: 25m 3s\n",
      "1500:\ttest: 0.1691224\tbest: 0.1692003 (1473)\ttotal: 4m 24s\tremaining: 24m 55s\n",
      "1550:\ttest: 0.1690158\tbest: 0.1692003 (1473)\ttotal: 4m 32s\tremaining: 24m 47s\n",
      "1600:\ttest: 0.1691432\tbest: 0.1692003 (1473)\ttotal: 4m 41s\tremaining: 24m 37s\n",
      "1650:\ttest: 0.1691706\tbest: 0.1692003 (1473)\ttotal: 4m 50s\tremaining: 24m 28s\n",
      "1700:\ttest: 0.1692594\tbest: 0.1693098 (1660)\ttotal: 4m 59s\tremaining: 24m 19s\n",
      "1750:\ttest: 0.1692294\tbest: 0.1694041 (1722)\ttotal: 5m 7s\tremaining: 24m 10s\n",
      "1800:\ttest: 0.1692832\tbest: 0.1694041 (1722)\ttotal: 5m 16s\tremaining: 24m 2s\n",
      "1850:\ttest: 0.1693617\tbest: 0.1694706 (1846)\ttotal: 5m 25s\tremaining: 23m 53s\n",
      "1900:\ttest: 0.1693788\tbest: 0.1694706 (1846)\ttotal: 5m 34s\tremaining: 23m 44s\n",
      "1950:\ttest: 0.1694918\tbest: 0.1695809 (1913)\ttotal: 5m 43s\tremaining: 23m 35s\n",
      "2000:\ttest: 0.1694774\tbest: 0.1696334 (1987)\ttotal: 5m 51s\tremaining: 23m 26s\n",
      "2050:\ttest: 0.1695503\tbest: 0.1696334 (1987)\ttotal: 6m\tremaining: 23m 17s\n",
      "2100:\ttest: 0.1695516\tbest: 0.1696334 (1987)\ttotal: 6m 9s\tremaining: 23m 8s\n",
      "2150:\ttest: 0.1696276\tbest: 0.1696532 (2148)\ttotal: 6m 18s\tremaining: 22m 59s\n",
      "2200:\ttest: 0.1697272\tbest: 0.1697593 (2189)\ttotal: 6m 26s\tremaining: 22m 50s\n",
      "2250:\ttest: 0.1697319\tbest: 0.1697986 (2238)\ttotal: 6m 35s\tremaining: 22m 41s\n",
      "2300:\ttest: 0.1695543\tbest: 0.1697986 (2238)\ttotal: 6m 44s\tremaining: 22m 32s\n",
      "2350:\ttest: 0.1694664\tbest: 0.1697986 (2238)\ttotal: 6m 53s\tremaining: 22m 24s\n",
      "2400:\ttest: 0.1693002\tbest: 0.1697986 (2238)\ttotal: 7m 1s\tremaining: 22m 15s\n",
      "2450:\ttest: 0.1693338\tbest: 0.1697986 (2238)\ttotal: 7m 10s\tremaining: 22m 6s\n",
      "2500:\ttest: 0.1693874\tbest: 0.1697986 (2238)\ttotal: 7m 19s\tremaining: 21m 57s\n",
      "2550:\ttest: 0.1694474\tbest: 0.1697986 (2238)\ttotal: 7m 27s\tremaining: 21m 48s\n",
      "2600:\ttest: 0.1697034\tbest: 0.1697986 (2238)\ttotal: 7m 36s\tremaining: 21m 39s\n",
      "bestTest = 0.1697986037\n",
      "bestIteration = 2238\n",
      "Shrink model to first 2239 iterations.\n",
      "Best iteration: 2238\n",
      "Using validation metric: NDCG:top=12;type=Base\n",
      "Training log saved successfully: ../data/outputs/catboost_training_log.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5 – GPU CatBoostRanker with eval logging\n",
    "\n",
    "MODEL_PATH = \"../data/outputs/catboost_ranker.model\"\n",
    "LOG_PATH = \"../data/outputs/catboost_training_log.txt\"\n",
    "\n",
    "model = CatBoostRanker(\n",
    "    loss_function='YetiRank',\n",
    "    eval_metric='NDCG:top=12',\n",
    "    iterations=10000,\n",
    "    learning_rate=0.13294997299885136,\n",
    "    depth=10,\n",
    "    l2_leaf_reg=1.0589957706654705,\n",
    "    min_data_in_leaf=21,\n",
    "    random_strength=0.6391996053429478,\n",
    "    bagging_temperature=0.1992110736721816,\n",
    "    border_count=172,\n",
    "    task_type='GPU',\n",
    "    devices='0',\n",
    "    random_seed=42,\n",
    "    early_stopping_rounds=400,\n",
    "    verbose=50,\n",
    ")\n",
    "\n",
    "model.fit(pool_train, eval_set=pool_valid)\n",
    "best_iter = model.get_best_iteration()\n",
    "print(\"Best iteration:\", best_iter)\n",
    "\n",
    "model.shrink(ntree_end=best_iter + 1)\n",
    "model.save_model(MODEL_PATH)\n",
    "\n",
    "\n",
    "evals = model.get_evals_result()\n",
    "\n",
    "# Find the key containing NDCG in validation set\n",
    "def find_ndcg_key(eval_dict):\n",
    "    for key in eval_dict:\n",
    "        if \"NDCG\" in key.upper():\n",
    "            return key\n",
    "    # If not found, return first available key or raise error\n",
    "    available = list(eval_dict.keys())\n",
    "    if available:\n",
    "        print(f\"Warning: NDCG not found. Using first available metric: {available[0]}\")\n",
    "        return available[0]\n",
    "    raise KeyError(f\"No metrics found in eval_dict. Available keys: {available}\")\n",
    "\n",
    "valid_metric_key = find_ndcg_key(evals['validation'])\n",
    "print(f\"Using validation metric: {valid_metric_key}\")\n",
    "\n",
    "# Since learn is empty, we'll only log validation metrics\n",
    "valid_log = evals['validation'][valid_metric_key]\n",
    "\n",
    "with open(LOG_PATH, \"w\") as f:\n",
    "    f.write(\"iter,valid_ndcg12\\n\")\n",
    "    for i, val in enumerate(valid_log):\n",
    "        f.write(f\"{i},{val}\\n\")\n",
    "\n",
    "print(\"Training log saved successfully:\", LOG_PATH)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ba382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6 – cleanup after training\n",
    "\n",
    "del pool_train, pool_valid\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stored model: ../data/outputs/catboost_ranker.model\n",
      "Model loaded successfully\n",
      "Using 34 features for inference\n",
      "Loading submission week features...\n",
      "Running inference on 250,982,495 rows...\n",
      "Predicted rows 0 to 2,000,000 / 250,982,495\n",
      "Predicted rows 2,000,000 to 4,000,000 / 250,982,495\n",
      "Predicted rows 4,000,000 to 6,000,000 / 250,982,495\n",
      "Predicted rows 6,000,000 to 8,000,000 / 250,982,495\n",
      "Predicted rows 8,000,000 to 10,000,000 / 250,982,495\n",
      "Predicted rows 10,000,000 to 12,000,000 / 250,982,495\n",
      "Predicted rows 12,000,000 to 14,000,000 / 250,982,495\n",
      "Predicted rows 14,000,000 to 16,000,000 / 250,982,495\n",
      "Predicted rows 16,000,000 to 18,000,000 / 250,982,495\n",
      "Predicted rows 18,000,000 to 20,000,000 / 250,982,495\n",
      "Predicted rows 20,000,000 to 22,000,000 / 250,982,495\n",
      "Predicted rows 22,000,000 to 24,000,000 / 250,982,495\n",
      "Predicted rows 24,000,000 to 26,000,000 / 250,982,495\n",
      "Predicted rows 26,000,000 to 28,000,000 / 250,982,495\n",
      "Predicted rows 28,000,000 to 30,000,000 / 250,982,495\n",
      "Predicted rows 30,000,000 to 32,000,000 / 250,982,495\n",
      "Predicted rows 32,000,000 to 34,000,000 / 250,982,495\n",
      "Predicted rows 34,000,000 to 36,000,000 / 250,982,495\n",
      "Predicted rows 36,000,000 to 38,000,000 / 250,982,495\n",
      "Predicted rows 38,000,000 to 40,000,000 / 250,982,495\n",
      "Predicted rows 40,000,000 to 42,000,000 / 250,982,495\n",
      "Predicted rows 42,000,000 to 44,000,000 / 250,982,495\n",
      "Predicted rows 44,000,000 to 46,000,000 / 250,982,495\n",
      "Predicted rows 46,000,000 to 48,000,000 / 250,982,495\n",
      "Predicted rows 48,000,000 to 50,000,000 / 250,982,495\n",
      "Predicted rows 50,000,000 to 52,000,000 / 250,982,495\n",
      "Predicted rows 52,000,000 to 54,000,000 / 250,982,495\n",
      "Predicted rows 54,000,000 to 56,000,000 / 250,982,495\n",
      "Predicted rows 56,000,000 to 58,000,000 / 250,982,495\n",
      "Predicted rows 58,000,000 to 60,000,000 / 250,982,495\n",
      "Predicted rows 60,000,000 to 62,000,000 / 250,982,495\n",
      "Predicted rows 62,000,000 to 64,000,000 / 250,982,495\n",
      "Predicted rows 64,000,000 to 66,000,000 / 250,982,495\n",
      "Predicted rows 66,000,000 to 68,000,000 / 250,982,495\n",
      "Predicted rows 68,000,000 to 70,000,000 / 250,982,495\n",
      "Predicted rows 70,000,000 to 72,000,000 / 250,982,495\n",
      "Predicted rows 72,000,000 to 74,000,000 / 250,982,495\n",
      "Predicted rows 74,000,000 to 76,000,000 / 250,982,495\n",
      "Predicted rows 76,000,000 to 78,000,000 / 250,982,495\n",
      "Predicted rows 78,000,000 to 80,000,000 / 250,982,495\n",
      "Predicted rows 80,000,000 to 82,000,000 / 250,982,495\n",
      "Predicted rows 82,000,000 to 84,000,000 / 250,982,495\n",
      "Predicted rows 84,000,000 to 86,000,000 / 250,982,495\n",
      "Predicted rows 86,000,000 to 88,000,000 / 250,982,495\n",
      "Predicted rows 88,000,000 to 90,000,000 / 250,982,495\n",
      "Predicted rows 90,000,000 to 92,000,000 / 250,982,495\n",
      "Predicted rows 92,000,000 to 94,000,000 / 250,982,495\n",
      "Predicted rows 94,000,000 to 96,000,000 / 250,982,495\n",
      "Predicted rows 96,000,000 to 98,000,000 / 250,982,495\n",
      "Predicted rows 98,000,000 to 100,000,000 / 250,982,495\n",
      "Predicted rows 100,000,000 to 102,000,000 / 250,982,495\n",
      "Predicted rows 102,000,000 to 104,000,000 / 250,982,495\n",
      "Predicted rows 104,000,000 to 106,000,000 / 250,982,495\n",
      "Predicted rows 106,000,000 to 108,000,000 / 250,982,495\n",
      "Predicted rows 108,000,000 to 110,000,000 / 250,982,495\n",
      "Predicted rows 110,000,000 to 112,000,000 / 250,982,495\n",
      "Predicted rows 112,000,000 to 114,000,000 / 250,982,495\n",
      "Predicted rows 114,000,000 to 116,000,000 / 250,982,495\n",
      "Predicted rows 116,000,000 to 118,000,000 / 250,982,495\n",
      "Predicted rows 118,000,000 to 120,000,000 / 250,982,495\n",
      "Predicted rows 120,000,000 to 122,000,000 / 250,982,495\n",
      "Predicted rows 122,000,000 to 124,000,000 / 250,982,495\n",
      "Predicted rows 124,000,000 to 126,000,000 / 250,982,495\n",
      "Predicted rows 126,000,000 to 128,000,000 / 250,982,495\n",
      "Predicted rows 128,000,000 to 130,000,000 / 250,982,495\n",
      "Predicted rows 130,000,000 to 132,000,000 / 250,982,495\n",
      "Predicted rows 132,000,000 to 134,000,000 / 250,982,495\n",
      "Predicted rows 134,000,000 to 136,000,000 / 250,982,495\n",
      "Predicted rows 136,000,000 to 138,000,000 / 250,982,495\n",
      "Predicted rows 138,000,000 to 140,000,000 / 250,982,495\n",
      "Predicted rows 140,000,000 to 142,000,000 / 250,982,495\n",
      "Predicted rows 142,000,000 to 144,000,000 / 250,982,495\n",
      "Predicted rows 144,000,000 to 146,000,000 / 250,982,495\n",
      "Predicted rows 146,000,000 to 148,000,000 / 250,982,495\n",
      "Predicted rows 148,000,000 to 150,000,000 / 250,982,495\n",
      "Predicted rows 150,000,000 to 152,000,000 / 250,982,495\n",
      "Predicted rows 152,000,000 to 154,000,000 / 250,982,495\n",
      "Predicted rows 154,000,000 to 156,000,000 / 250,982,495\n",
      "Predicted rows 156,000,000 to 158,000,000 / 250,982,495\n",
      "Predicted rows 158,000,000 to 160,000,000 / 250,982,495\n",
      "Predicted rows 160,000,000 to 162,000,000 / 250,982,495\n",
      "Predicted rows 162,000,000 to 164,000,000 / 250,982,495\n",
      "Predicted rows 164,000,000 to 166,000,000 / 250,982,495\n",
      "Predicted rows 166,000,000 to 168,000,000 / 250,982,495\n",
      "Predicted rows 168,000,000 to 170,000,000 / 250,982,495\n",
      "Predicted rows 170,000,000 to 172,000,000 / 250,982,495\n",
      "Predicted rows 172,000,000 to 174,000,000 / 250,982,495\n",
      "Predicted rows 174,000,000 to 176,000,000 / 250,982,495\n",
      "Predicted rows 176,000,000 to 178,000,000 / 250,982,495\n",
      "Predicted rows 178,000,000 to 180,000,000 / 250,982,495\n",
      "Predicted rows 180,000,000 to 182,000,000 / 250,982,495\n",
      "Predicted rows 182,000,000 to 184,000,000 / 250,982,495\n",
      "Predicted rows 184,000,000 to 186,000,000 / 250,982,495\n",
      "Predicted rows 186,000,000 to 188,000,000 / 250,982,495\n",
      "Predicted rows 188,000,000 to 190,000,000 / 250,982,495\n",
      "Predicted rows 190,000,000 to 192,000,000 / 250,982,495\n",
      "Predicted rows 192,000,000 to 194,000,000 / 250,982,495\n",
      "Predicted rows 194,000,000 to 196,000,000 / 250,982,495\n",
      "Predicted rows 196,000,000 to 198,000,000 / 250,982,495\n",
      "Predicted rows 198,000,000 to 200,000,000 / 250,982,495\n",
      "Predicted rows 200,000,000 to 202,000,000 / 250,982,495\n",
      "Predicted rows 202,000,000 to 204,000,000 / 250,982,495\n",
      "Predicted rows 204,000,000 to 206,000,000 / 250,982,495\n",
      "Predicted rows 206,000,000 to 208,000,000 / 250,982,495\n",
      "Predicted rows 208,000,000 to 210,000,000 / 250,982,495\n",
      "Predicted rows 210,000,000 to 212,000,000 / 250,982,495\n",
      "Predicted rows 212,000,000 to 214,000,000 / 250,982,495\n",
      "Predicted rows 214,000,000 to 216,000,000 / 250,982,495\n",
      "Predicted rows 216,000,000 to 218,000,000 / 250,982,495\n",
      "Predicted rows 218,000,000 to 220,000,000 / 250,982,495\n",
      "Predicted rows 220,000,000 to 222,000,000 / 250,982,495\n",
      "Predicted rows 222,000,000 to 224,000,000 / 250,982,495\n",
      "Predicted rows 224,000,000 to 226,000,000 / 250,982,495\n",
      "Predicted rows 226,000,000 to 228,000,000 / 250,982,495\n",
      "Predicted rows 228,000,000 to 230,000,000 / 250,982,495\n",
      "Predicted rows 230,000,000 to 232,000,000 / 250,982,495\n",
      "Predicted rows 232,000,000 to 234,000,000 / 250,982,495\n",
      "Predicted rows 234,000,000 to 236,000,000 / 250,982,495\n",
      "Predicted rows 236,000,000 to 238,000,000 / 250,982,495\n",
      "Predicted rows 238,000,000 to 240,000,000 / 250,982,495\n",
      "Predicted rows 240,000,000 to 242,000,000 / 250,982,495\n",
      "Predicted rows 242,000,000 to 244,000,000 / 250,982,495\n",
      "Predicted rows 244,000,000 to 246,000,000 / 250,982,495\n",
      "Predicted rows 246,000,000 to 248,000,000 / 250,982,495\n",
      "Predicted rows 248,000,000 to 250,000,000 / 250,982,495\n",
      "Predicted rows 250,000,000 to 250,982,495 / 250,982,495\n",
      "Final scored rows: 250982495\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 – inference on submission week\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRanker, Pool\n",
    "\n",
    "MODEL_PATH = \"../data/outputs/catboost_ranker.model\"\n",
    "META_PATH = '../data/outputs/dataset_meta.json'\n",
    "FEATURES_PATH = \"../data/outputs/features_week=20200922.parquet\"\n",
    "\n",
    "print(\"Loading stored model:\", MODEL_PATH)\n",
    "model = CatBoostRanker()\n",
    "model.load_model(MODEL_PATH)\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Load feature_cols from metadata\n",
    "with open(META_PATH) as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "feature_cols = meta['model_features']\n",
    "\n",
    "# Apply same feature removal as training\n",
    "remove_features = [\n",
    "    # 'value',\n",
    "    # 'age_bucket',\n",
    "    # 'customer_total_purchases',\n",
    "    # 'customer_unique_articles',\n",
    "    # 'cust_purchases_1w',\n",
    "    # 'cust_purchases_4w',\n",
    "    # 'article_total_purchases',\n",
    "    # 'article_unique_customers',\n",
    "    #'club_member_status',\n",
    "    #'fashion_news_frequency',\n",
    "    #'age',\n",
    "    #'postal_code',\n",
    "    #'product_code',\n",
    "    #'product_type_no',\n",
    "    #'product_group_name',\n",
    "    #'graphical_appearance_no',\n",
    "    #'colour_group_code',\n",
    "    #'perceived_colour_value_id',\n",
    "    #'perceived_colour_master_id',\n",
    "    #'department_no',\n",
    "    #'index_code',\n",
    "    #'index_group_no',\n",
    "    #'section_no',\n",
    "    #'garment_group_no',\n",
    "    # 'article_mean_price',\n",
    "    # 'customer_mean_price',\n",
    "    # 'article_mean_age',\n",
    "    # 'index_group_match',\n",
    "    # 'product_code_match',\n",
    "    # 'days_since_last_purchase',\n",
    "    # 'customer_days_since_last_purchase',\n",
    "    # 'price_sensitivity',\n",
    "    # 'age_sensitivity',\n",
    "    # 'window_type_code',\n",
    "]\n",
    "\n",
    "feature_cols = [c for c in feature_cols if c not in remove_features]\n",
    "cat_features = []\n",
    "\n",
    "print(f\"Using {len(feature_cols)} features for inference\")\n",
    "print(\"Loading submission week features...\")\n",
    "data = pd.read_parquet(\n",
    "    FEATURES_PATH,\n",
    "    columns=[\"customer_id\", \"article_id\"] + feature_cols\n",
    ")\n",
    "\n",
    "data[\"customer_id\"] = data[\"customer_id\"].astype(\"int64\")\n",
    "data[\"article_id\"] = data[\"article_id\"].astype(\"int32\")\n",
    "\n",
    "for c in feature_cols:\n",
    "    if pd.api.types.is_float_dtype(data[c]):\n",
    "        data[c] = data[c].astype(\"float32\")\n",
    "\n",
    "BATCH = 2_000_000\n",
    "n_rows = len(data)\n",
    "scores = np.empty(n_rows, dtype=np.float32)\n",
    "\n",
    "print(f\"Running inference on {n_rows:,} rows...\")\n",
    "\n",
    "for start in range(0, n_rows, BATCH):\n",
    "    end = min(start + BATCH, n_rows)\n",
    "    infer_pool = Pool(\n",
    "        data=data.iloc[start:end][feature_cols],\n",
    "        cat_features=cat_features\n",
    "    )\n",
    "    scores[start:end] = model.predict(infer_pool).astype(np.float32)\n",
    "    del infer_pool\n",
    "    gc.collect()\n",
    "    print(f\"Predicted rows {start:,} to {end:,} / {n_rows:,}\")\n",
    "\n",
    "data[\"score\"] = scores\n",
    "del scores\n",
    "gc.collect()\n",
    "\n",
    "print(\"Final scored rows:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503d737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4431/3627138227.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top12[\"article_id_str\"] = top12[\"article_id\"].astype(str).str.zfill(10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions built: 1371980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 8 – top-12 per customer\n",
    "\n",
    "data = data.sort_values([\"customer_id\", \"score\"], ascending=[True, False])\n",
    "top12 = data.groupby(\"customer_id\", group_keys=False).head(12)\n",
    "\n",
    "top12[\"article_id_str\"] = top12[\"article_id\"].astype(str).str.zfill(10)\n",
    "\n",
    "pred_df = (\n",
    "    top12.groupby(\"customer_id\")[\"article_id_str\"]\n",
    "         .apply(lambda x: \" \".join(x))\n",
    "         .reset_index()\n",
    "         .rename(columns={\"customer_id\": \"customer_id_int\",\n",
    "                          \"article_id_str\": \"prediction\"})\n",
    ")\n",
    "\n",
    "pred_df[\"customer_id_int\"] = pred_df[\"customer_id_int\"].astype(\"int64\")\n",
    "\n",
    "print(\"Predictions built:\", len(pred_df))\n",
    "\n",
    "del top12, data\n",
    "gc.collect()\n",
    "\n",
    "# 10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b4a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample customers: 1371980\n",
      "Predicted customers: 1371980\n",
      "Predictions matched: 1371980\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 – merge with sample + fallback\n",
    "SAMPLE_PATH = '../data/input_data/sample_submission.csv'\n",
    "GENERAL_PATH = '../data/outputs/general_pred_str.json'\n",
    "\n",
    "\n",
    "def hex16_to_int(s):\n",
    "    return np.int64(np.uint64(int(s[-16:], 16)))\n",
    "\n",
    "sample = pd.read_csv(SAMPLE_PATH)\n",
    "sample[\"customer_id_int\"] = sample[\"customer_id\"].apply(hex16_to_int)\n",
    "sample = sample.drop(columns=[\"prediction\"], errors=\"ignore\")\n",
    "\n",
    "pred_df[\"customer_id_int\"] = pred_df[\"customer_id_int\"].astype(\"int64\")\n",
    "\n",
    "print(\"Sample customers:\", sample[\"customer_id_int\"].nunique())\n",
    "print(\"Predicted customers:\", pred_df[\"customer_id_int\"].nunique())\n",
    "\n",
    "sub = sample.merge(pred_df, how=\"left\", on=\"customer_id_int\")\n",
    "print(\"Predictions matched:\", sub[\"prediction\"].notna().sum())\n",
    "\n",
    "del sample, pred_df\n",
    "gc.collect()\n",
    "\n",
    "gp = json.load(open(GENERAL_PATH))\n",
    "fallback_str = gp[\"general_pred_str\"]\n",
    "fallback_items = fallback_str.split()\n",
    "\n",
    "sub[\"prediction\"] = sub[\"prediction\"].fillna(fallback_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992884f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to: ../data/submission/catboost_ranker_submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_id_int</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>6883939031699146327</td>\n",
       "      <td>0568601006 0568601043 0568601044 0568601007 08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>-7200416642310594310</td>\n",
       "      <td>0673677002 0448509014 0918522001 0918525001 09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>-6846340800584936</td>\n",
       "      <td>0794321007 0794321011 0794321008 0851400020 08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n",
       "      <td>-94071612138601410</td>\n",
       "      <td>0794321011 0804992017 0805000001 0730683050 07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n",
       "      <td>-283965518499174310</td>\n",
       "      <td>0896152002 0730683050 0730683062 0791587001 08...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id      customer_id_int  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...  6883939031699146327   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e... -7200416642310594310   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...    -6846340800584936   \n",
       "3  00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...   -94071612138601410   \n",
       "4  00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...  -283965518499174310   \n",
       "\n",
       "                                          prediction  \n",
       "0  0568601006 0568601043 0568601044 0568601007 08...  \n",
       "1  0673677002 0448509014 0918522001 0918525001 09...  \n",
       "2  0794321007 0794321011 0794321008 0851400020 08...  \n",
       "3  0794321011 0804992017 0805000001 0730683050 07...  \n",
       "4  0896152002 0730683050 0730683062 0791587001 08...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 10 – pad to 12 and save\n",
    "SUB_PATH = '../data/submission/catboost_ranker_submission.csv'\n",
    "\n",
    "\n",
    "def pad_to_12(pred):\n",
    "    items = pred.split()\n",
    "    if len(items) >= 12:\n",
    "        return \" \".join(items[:12])\n",
    "    seen = set(items)\n",
    "    for art in fallback_items:\n",
    "        if art not in seen:\n",
    "            items.append(art)\n",
    "            seen.add(art)\n",
    "        if len(items) == 12:\n",
    "            break\n",
    "    return \" \".join(items)\n",
    "\n",
    "sub[\"prediction\"] = sub[\"prediction\"].apply(pad_to_12)\n",
    "\n",
    "sub[[\"customer_id\", \"prediction\"]].to_csv(SUB_PATH, index=False)\n",
    "print(\"Submission saved to:\", SUB_PATH)\n",
    "\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
