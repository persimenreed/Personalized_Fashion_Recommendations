{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5758312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 – imports & config\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "META_PATH = '../data/outputs/dataset_meta.json'\n",
    "TRAIN_RANK_PATH = '../data/outputs/train_rank.parquet'\n",
    "VALID_RANK_PATH = '../data/outputs/valid_rank.parquet'\n",
    "GROUP_TRAIN_PATH = '../data/outputs/groups_train.npy'\n",
    "GROUP_VALID_PATH = '../data/outputs/groups_valid.npy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6baafddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 8180082\n",
      "Valid rows: 253714158\n",
      "Features: 34\n",
      "Groups train: 294983 valid: 1371980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2 – load train/valid ranking data\n",
    "with open(META_PATH) as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "feature_cols = meta['model_features']\n",
    "\n",
    "train_df = pd.read_parquet(TRAIN_RANK_PATH)\n",
    "valid_df = pd.read_parquet(VALID_RANK_PATH)\n",
    "\n",
    "# REMOVE FEATURES – must match LGBM\n",
    "remove_features = [\n",
    "    # 'value',\n",
    "    # 'age_bucket',\n",
    "    # 'customer_total_purchases',\n",
    "    # 'customer_unique_articles',\n",
    "    # 'cust_purchases_1w',\n",
    "    # 'cust_purchases_4w',\n",
    "    # 'article_total_purchases',\n",
    "    # 'article_unique_customers',\n",
    "    #'club_member_status',\n",
    "    #'fashion_news_frequency',\n",
    "    #'age',\n",
    "    #'postal_code',\n",
    "    #'product_code',\n",
    "    #'product_type_no',\n",
    "    #'product_group_name',\n",
    "    #'graphical_appearance_no',\n",
    "    #'colour_group_code',\n",
    "    #'perceived_colour_value_id',\n",
    "    #'perceived_colour_master_id',\n",
    "    #'department_no',\n",
    "    #'index_code',\n",
    "    #'index_group_no',\n",
    "    #'section_no',\n",
    "    #'garment_group_no',\n",
    "    # 'article_mean_price',\n",
    "    # 'customer_mean_price',\n",
    "    # 'article_mean_age',\n",
    "    # 'index_group_match',\n",
    "    # 'product_code_match',\n",
    "    # 'days_since_last_purchase',\n",
    "    # 'customer_days_since_last_purchase',\n",
    "    # 'price_sensitivity',\n",
    "    # 'age_sensitivity',\n",
    "    # 'window_type_code',\n",
    "]\n",
    "\n",
    "drop_cols = [c for c in remove_features if c in train_df.columns]\n",
    "if drop_cols:\n",
    "    print(\"Dropping from TRAIN/VALID:\", drop_cols)\n",
    "    train_df = train_df.drop(columns=drop_cols)\n",
    "    valid_df = valid_df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# Update feature_cols identically\n",
    "feature_cols = [f for f in feature_cols if f not in remove_features]\n",
    "\n",
    "train_group = np.load(GROUP_TRAIN_PATH)\n",
    "valid_group = np.load(GROUP_VALID_PATH)\n",
    "\n",
    "# Float32 conversion for XGBoost\n",
    "for c in feature_cols:\n",
    "    if pd.api.types.is_float_dtype(train_df[c]):\n",
    "        train_df[c] = train_df[c].astype('float32')\n",
    "        valid_df[c] = valid_df[c].astype('float32')\n",
    "\n",
    "train_df['label'] = train_df['label'].astype('float32')\n",
    "valid_df['label'] = valid_df['label'].astype('float32')\n",
    "\n",
    "print(\"Train rows:\", len(train_df))\n",
    "print(\"Valid rows:\", len(valid_df))\n",
    "print(\"Features:\", len(feature_cols))\n",
    "print(\"Groups train:\", len(train_group), \"valid:\", len(valid_group))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "#40s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef0b9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering validation set to groups with at least 1 positive...\n",
      "Filtered valid groups: 27802\n",
      "Filtered valid rows: 8372080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 3 – filter validation groups with ≥1 positive (match LightGBM)\n",
    "\n",
    "print(\"\\nFiltering validation set to groups with at least 1 positive...\")\n",
    "\n",
    "# Use original valid_group from disk (same as LGBM)\n",
    "orig_valid_group = np.load(GROUP_VALID_PATH)\n",
    "group_bounds = np.insert(np.cumsum(orig_valid_group), 0, 0)[:-1]\n",
    "group_has_positive = np.add.reduceat(valid_df['label'].values, group_bounds) > 0\n",
    "\n",
    "valid_group = orig_valid_group[group_has_positive]\n",
    "row_mask = np.repeat(group_has_positive, orig_valid_group)\n",
    "\n",
    "# Apply mask to validation dataframe\n",
    "valid_df = valid_df[row_mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Filtered valid groups:\", len(valid_group))\n",
    "print(\"Filtered valid rows:\", len(valid_df))\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caeb4836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-ndcg@12:0.78060\tvalid-ndcg@12:0.13983\n",
      "[25]\ttrain-ndcg@12:0.79501\tvalid-ndcg@12:0.15716\n",
      "[50]\ttrain-ndcg@12:0.79641\tvalid-ndcg@12:0.15791\n",
      "[75]\ttrain-ndcg@12:0.79784\tvalid-ndcg@12:0.15868\n",
      "[100]\ttrain-ndcg@12:0.79886\tvalid-ndcg@12:0.15901\n",
      "[125]\ttrain-ndcg@12:0.80001\tvalid-ndcg@12:0.15981\n",
      "[150]\ttrain-ndcg@12:0.80115\tvalid-ndcg@12:0.16085\n",
      "[175]\ttrain-ndcg@12:0.80207\tvalid-ndcg@12:0.16175\n",
      "[200]\ttrain-ndcg@12:0.80286\tvalid-ndcg@12:0.16246\n",
      "[225]\ttrain-ndcg@12:0.80369\tvalid-ndcg@12:0.16327\n",
      "[250]\ttrain-ndcg@12:0.80442\tvalid-ndcg@12:0.16341\n",
      "[275]\ttrain-ndcg@12:0.80524\tvalid-ndcg@12:0.16398\n",
      "[300]\ttrain-ndcg@12:0.80596\tvalid-ndcg@12:0.16459\n",
      "[325]\ttrain-ndcg@12:0.80664\tvalid-ndcg@12:0.16483\n",
      "[350]\ttrain-ndcg@12:0.80729\tvalid-ndcg@12:0.16533\n",
      "[375]\ttrain-ndcg@12:0.80809\tvalid-ndcg@12:0.16559\n",
      "[400]\ttrain-ndcg@12:0.80879\tvalid-ndcg@12:0.16582\n",
      "[425]\ttrain-ndcg@12:0.80952\tvalid-ndcg@12:0.16625\n",
      "[450]\ttrain-ndcg@12:0.81011\tvalid-ndcg@12:0.16611\n",
      "[475]\ttrain-ndcg@12:0.81066\tvalid-ndcg@12:0.16648\n",
      "[500]\ttrain-ndcg@12:0.81127\tvalid-ndcg@12:0.16664\n",
      "[525]\ttrain-ndcg@12:0.81184\tvalid-ndcg@12:0.16676\n",
      "[550]\ttrain-ndcg@12:0.81231\tvalid-ndcg@12:0.16702\n",
      "[575]\ttrain-ndcg@12:0.81284\tvalid-ndcg@12:0.16712\n",
      "[600]\ttrain-ndcg@12:0.81339\tvalid-ndcg@12:0.16735\n",
      "[625]\ttrain-ndcg@12:0.81388\tvalid-ndcg@12:0.16744\n",
      "[650]\ttrain-ndcg@12:0.81435\tvalid-ndcg@12:0.16757\n",
      "[675]\ttrain-ndcg@12:0.81485\tvalid-ndcg@12:0.16766\n",
      "[700]\ttrain-ndcg@12:0.81531\tvalid-ndcg@12:0.16783\n",
      "[725]\ttrain-ndcg@12:0.81569\tvalid-ndcg@12:0.16807\n",
      "[750]\ttrain-ndcg@12:0.81609\tvalid-ndcg@12:0.16808\n",
      "[775]\ttrain-ndcg@12:0.81642\tvalid-ndcg@12:0.16815\n",
      "[800]\ttrain-ndcg@12:0.81673\tvalid-ndcg@12:0.16834\n",
      "[825]\ttrain-ndcg@12:0.81706\tvalid-ndcg@12:0.16851\n",
      "[850]\ttrain-ndcg@12:0.81735\tvalid-ndcg@12:0.16862\n",
      "[875]\ttrain-ndcg@12:0.81763\tvalid-ndcg@12:0.16880\n",
      "[900]\ttrain-ndcg@12:0.81795\tvalid-ndcg@12:0.16892\n",
      "[925]\ttrain-ndcg@12:0.81823\tvalid-ndcg@12:0.16903\n",
      "[950]\ttrain-ndcg@12:0.81851\tvalid-ndcg@12:0.16897\n",
      "[975]\ttrain-ndcg@12:0.81882\tvalid-ndcg@12:0.16891\n",
      "[1000]\ttrain-ndcg@12:0.81915\tvalid-ndcg@12:0.16887\n",
      "[1025]\ttrain-ndcg@12:0.81945\tvalid-ndcg@12:0.16891\n",
      "[1050]\ttrain-ndcg@12:0.81966\tvalid-ndcg@12:0.16891\n",
      "[1075]\ttrain-ndcg@12:0.81994\tvalid-ndcg@12:0.16903\n",
      "[1100]\ttrain-ndcg@12:0.82021\tvalid-ndcg@12:0.16912\n",
      "[1125]\ttrain-ndcg@12:0.82054\tvalid-ndcg@12:0.16927\n",
      "[1150]\ttrain-ndcg@12:0.82083\tvalid-ndcg@12:0.16925\n",
      "[1175]\ttrain-ndcg@12:0.82111\tvalid-ndcg@12:0.16932\n",
      "[1200]\ttrain-ndcg@12:0.82137\tvalid-ndcg@12:0.16954\n",
      "[1225]\ttrain-ndcg@12:0.82163\tvalid-ndcg@12:0.16954\n",
      "[1250]\ttrain-ndcg@12:0.82186\tvalid-ndcg@12:0.16947\n",
      "[1275]\ttrain-ndcg@12:0.82205\tvalid-ndcg@12:0.16948\n",
      "[1300]\ttrain-ndcg@12:0.82236\tvalid-ndcg@12:0.16958\n",
      "[1325]\ttrain-ndcg@12:0.82258\tvalid-ndcg@12:0.16970\n",
      "[1350]\ttrain-ndcg@12:0.82286\tvalid-ndcg@12:0.16991\n",
      "[1375]\ttrain-ndcg@12:0.82305\tvalid-ndcg@12:0.16974\n",
      "[1400]\ttrain-ndcg@12:0.82331\tvalid-ndcg@12:0.16960\n",
      "[1425]\ttrain-ndcg@12:0.82353\tvalid-ndcg@12:0.16969\n",
      "[1450]\ttrain-ndcg@12:0.82377\tvalid-ndcg@12:0.16980\n",
      "[1475]\ttrain-ndcg@12:0.82398\tvalid-ndcg@12:0.16985\n",
      "[1500]\ttrain-ndcg@12:0.82422\tvalid-ndcg@12:0.16980\n",
      "[1525]\ttrain-ndcg@12:0.82436\tvalid-ndcg@12:0.16988\n",
      "[1550]\ttrain-ndcg@12:0.82458\tvalid-ndcg@12:0.16983\n",
      "[1575]\ttrain-ndcg@12:0.82480\tvalid-ndcg@12:0.17001\n",
      "[1600]\ttrain-ndcg@12:0.82501\tvalid-ndcg@12:0.16999\n",
      "[1625]\ttrain-ndcg@12:0.82520\tvalid-ndcg@12:0.17012\n",
      "[1650]\ttrain-ndcg@12:0.82539\tvalid-ndcg@12:0.16999\n",
      "[1675]\ttrain-ndcg@12:0.82560\tvalid-ndcg@12:0.17003\n",
      "[1700]\ttrain-ndcg@12:0.82577\tvalid-ndcg@12:0.17003\n",
      "[1725]\ttrain-ndcg@12:0.82599\tvalid-ndcg@12:0.17004\n",
      "[1750]\ttrain-ndcg@12:0.82625\tvalid-ndcg@12:0.17003\n",
      "[1775]\ttrain-ndcg@12:0.82644\tvalid-ndcg@12:0.17020\n",
      "[1800]\ttrain-ndcg@12:0.82662\tvalid-ndcg@12:0.17027\n",
      "[1825]\ttrain-ndcg@12:0.82680\tvalid-ndcg@12:0.17024\n",
      "[1850]\ttrain-ndcg@12:0.82707\tvalid-ndcg@12:0.17024\n",
      "[1875]\ttrain-ndcg@12:0.82725\tvalid-ndcg@12:0.17033\n",
      "[1900]\ttrain-ndcg@12:0.82744\tvalid-ndcg@12:0.17034\n",
      "[1925]\ttrain-ndcg@12:0.82766\tvalid-ndcg@12:0.17015\n",
      "[1950]\ttrain-ndcg@12:0.82784\tvalid-ndcg@12:0.17023\n",
      "[1975]\ttrain-ndcg@12:0.82801\tvalid-ndcg@12:0.17050\n",
      "[2000]\ttrain-ndcg@12:0.82820\tvalid-ndcg@12:0.17032\n",
      "[2025]\ttrain-ndcg@12:0.82840\tvalid-ndcg@12:0.17041\n",
      "[2050]\ttrain-ndcg@12:0.82857\tvalid-ndcg@12:0.17036\n",
      "[2075]\ttrain-ndcg@12:0.82879\tvalid-ndcg@12:0.17052\n",
      "[2100]\ttrain-ndcg@12:0.82897\tvalid-ndcg@12:0.17065\n",
      "[2125]\ttrain-ndcg@12:0.82912\tvalid-ndcg@12:0.17057\n",
      "[2150]\ttrain-ndcg@12:0.82933\tvalid-ndcg@12:0.17052\n",
      "[2175]\ttrain-ndcg@12:0.82955\tvalid-ndcg@12:0.17056\n",
      "[2200]\ttrain-ndcg@12:0.82970\tvalid-ndcg@12:0.17047\n",
      "[2225]\ttrain-ndcg@12:0.82989\tvalid-ndcg@12:0.17055\n",
      "[2250]\ttrain-ndcg@12:0.83004\tvalid-ndcg@12:0.17052\n",
      "[2275]\ttrain-ndcg@12:0.83022\tvalid-ndcg@12:0.17062\n",
      "[2300]\ttrain-ndcg@12:0.83042\tvalid-ndcg@12:0.17062\n",
      "[2306]\ttrain-ndcg@12:0.83045\tvalid-ndcg@12:0.17056\n",
      "Best iteration: 2106\n",
      "Best valid ndcg@12: 0.17069698942186845\n",
      "Model saved to: ../data/outputs/xgb_ranker.model\n",
      "Saved training log to: ../data/outputs/xgb_training_log.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_240852/1280936849.py:42: UserWarning: [14:20:05] WARNING: /workspace/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
      "  bst.save_model(MODEL_PATH)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 4 – always train XGBoost ranker and save model\n",
    "\n",
    "MODEL_PATH = \"../data/outputs/xgb_ranker.model\"\n",
    "\n",
    "dtrain = xgb.DMatrix(train_df[feature_cols], label=train_df['label'])\n",
    "dvalid = xgb.DMatrix(valid_df[feature_cols], label=valid_df['label'])\n",
    "\n",
    "dtrain.set_group(train_group)\n",
    "dvalid.set_group(valid_group)\n",
    "\n",
    "params = {\n",
    "    'objective': 'rank:ndcg',\n",
    "    'eval_metric': 'ndcg@12',\n",
    "    'eta': 0.03,\n",
    "    'max_depth': 8,\n",
    "    'min_child_weight': 150,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'lambda': 2.0,\n",
    "    'alpha': 0.0,\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "\n",
    "evals_result = {}\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=3000,\n",
    "    evals=[(dtrain, 'train'), (dvalid, 'valid')],\n",
    "    early_stopping_rounds=200,\n",
    "    evals_result=evals_result,\n",
    "    verbose_eval=25,\n",
    ")\n",
    "\n",
    "print(\"Best iteration:\", bst.best_iteration)\n",
    "print(\"Best valid ndcg@12:\", evals_result['valid']['ndcg@12'][bst.best_iteration])\n",
    "\n",
    "# Always save the trained model\n",
    "bst.save_model(MODEL_PATH)\n",
    "print(\"Model saved to:\", MODEL_PATH)\n",
    "\n",
    "# ==========================================================\n",
    "# Save training log to TXT\n",
    "# ==========================================================\n",
    "log_path = \"../data/outputs/xgb_training_log.txt\"\n",
    "\n",
    "train_log = evals_result.get('train', {}).get('ndcg@12', [])\n",
    "valid_log = evals_result.get('valid', {}).get('ndcg@12', [])\n",
    "\n",
    "max_len = min(len(train_log), len(valid_log))\n",
    "\n",
    "with open(log_path, \"w\") as f:\n",
    "    f.write(\"iter,train_ndcg12,valid_ndcg12\\n\")\n",
    "    for i in range(max_len):\n",
    "        f.write(f\"{i},{train_log[i]},{valid_log[i]}\\n\")\n",
    "\n",
    "print(\"Saved training log to:\", log_path)\n",
    "\n",
    "# Free memory\n",
    "del dtrain, dvalid, bst\n",
    "gc.collect()\n",
    "\n",
    "# 10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0409e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stored model: ../data/outputs/xgb_ranker.model\n",
      "Model loaded, best iteration: 2106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_240852/3871797005.py:8: UserWarning: [14:22:48] WARNING: /workspace/src/c_api/c_api.cc:1511: Unknown file format: `model`. Using UBJSON (`ubj`) as a guess.\n",
      "  bst.load_model(MODEL_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on 250,982,495 rows...\n",
      "Predicted rows 0 to 2,000,000 / 250,982,495\n",
      "Predicted rows 2,000,000 to 4,000,000 / 250,982,495\n",
      "Predicted rows 4,000,000 to 6,000,000 / 250,982,495\n",
      "Predicted rows 6,000,000 to 8,000,000 / 250,982,495\n",
      "Predicted rows 8,000,000 to 10,000,000 / 250,982,495\n",
      "Predicted rows 10,000,000 to 12,000,000 / 250,982,495\n",
      "Predicted rows 12,000,000 to 14,000,000 / 250,982,495\n",
      "Predicted rows 14,000,000 to 16,000,000 / 250,982,495\n",
      "Predicted rows 16,000,000 to 18,000,000 / 250,982,495\n",
      "Predicted rows 18,000,000 to 20,000,000 / 250,982,495\n",
      "Predicted rows 20,000,000 to 22,000,000 / 250,982,495\n",
      "Predicted rows 22,000,000 to 24,000,000 / 250,982,495\n",
      "Predicted rows 24,000,000 to 26,000,000 / 250,982,495\n",
      "Predicted rows 26,000,000 to 28,000,000 / 250,982,495\n",
      "Predicted rows 28,000,000 to 30,000,000 / 250,982,495\n",
      "Predicted rows 30,000,000 to 32,000,000 / 250,982,495\n",
      "Predicted rows 32,000,000 to 34,000,000 / 250,982,495\n",
      "Predicted rows 34,000,000 to 36,000,000 / 250,982,495\n",
      "Predicted rows 36,000,000 to 38,000,000 / 250,982,495\n",
      "Predicted rows 38,000,000 to 40,000,000 / 250,982,495\n",
      "Predicted rows 40,000,000 to 42,000,000 / 250,982,495\n",
      "Predicted rows 42,000,000 to 44,000,000 / 250,982,495\n",
      "Predicted rows 44,000,000 to 46,000,000 / 250,982,495\n",
      "Predicted rows 46,000,000 to 48,000,000 / 250,982,495\n",
      "Predicted rows 48,000,000 to 50,000,000 / 250,982,495\n",
      "Predicted rows 50,000,000 to 52,000,000 / 250,982,495\n",
      "Predicted rows 52,000,000 to 54,000,000 / 250,982,495\n",
      "Predicted rows 54,000,000 to 56,000,000 / 250,982,495\n",
      "Predicted rows 56,000,000 to 58,000,000 / 250,982,495\n",
      "Predicted rows 58,000,000 to 60,000,000 / 250,982,495\n",
      "Predicted rows 60,000,000 to 62,000,000 / 250,982,495\n",
      "Predicted rows 62,000,000 to 64,000,000 / 250,982,495\n",
      "Predicted rows 64,000,000 to 66,000,000 / 250,982,495\n",
      "Predicted rows 66,000,000 to 68,000,000 / 250,982,495\n",
      "Predicted rows 68,000,000 to 70,000,000 / 250,982,495\n",
      "Predicted rows 70,000,000 to 72,000,000 / 250,982,495\n",
      "Predicted rows 72,000,000 to 74,000,000 / 250,982,495\n",
      "Predicted rows 74,000,000 to 76,000,000 / 250,982,495\n",
      "Predicted rows 76,000,000 to 78,000,000 / 250,982,495\n",
      "Predicted rows 78,000,000 to 80,000,000 / 250,982,495\n",
      "Predicted rows 80,000,000 to 82,000,000 / 250,982,495\n",
      "Predicted rows 82,000,000 to 84,000,000 / 250,982,495\n",
      "Predicted rows 84,000,000 to 86,000,000 / 250,982,495\n",
      "Predicted rows 86,000,000 to 88,000,000 / 250,982,495\n",
      "Predicted rows 88,000,000 to 90,000,000 / 250,982,495\n",
      "Predicted rows 90,000,000 to 92,000,000 / 250,982,495\n",
      "Predicted rows 92,000,000 to 94,000,000 / 250,982,495\n",
      "Predicted rows 94,000,000 to 96,000,000 / 250,982,495\n",
      "Predicted rows 96,000,000 to 98,000,000 / 250,982,495\n",
      "Predicted rows 98,000,000 to 100,000,000 / 250,982,495\n",
      "Predicted rows 100,000,000 to 102,000,000 / 250,982,495\n",
      "Predicted rows 102,000,000 to 104,000,000 / 250,982,495\n",
      "Predicted rows 104,000,000 to 106,000,000 / 250,982,495\n",
      "Predicted rows 106,000,000 to 108,000,000 / 250,982,495\n",
      "Predicted rows 108,000,000 to 110,000,000 / 250,982,495\n",
      "Predicted rows 110,000,000 to 112,000,000 / 250,982,495\n",
      "Predicted rows 112,000,000 to 114,000,000 / 250,982,495\n",
      "Predicted rows 114,000,000 to 116,000,000 / 250,982,495\n",
      "Predicted rows 116,000,000 to 118,000,000 / 250,982,495\n",
      "Predicted rows 118,000,000 to 120,000,000 / 250,982,495\n",
      "Predicted rows 120,000,000 to 122,000,000 / 250,982,495\n",
      "Predicted rows 122,000,000 to 124,000,000 / 250,982,495\n",
      "Predicted rows 124,000,000 to 126,000,000 / 250,982,495\n",
      "Predicted rows 126,000,000 to 128,000,000 / 250,982,495\n",
      "Predicted rows 128,000,000 to 130,000,000 / 250,982,495\n",
      "Predicted rows 130,000,000 to 132,000,000 / 250,982,495\n",
      "Predicted rows 132,000,000 to 134,000,000 / 250,982,495\n",
      "Predicted rows 134,000,000 to 136,000,000 / 250,982,495\n",
      "Predicted rows 136,000,000 to 138,000,000 / 250,982,495\n",
      "Predicted rows 138,000,000 to 140,000,000 / 250,982,495\n",
      "Predicted rows 140,000,000 to 142,000,000 / 250,982,495\n",
      "Predicted rows 142,000,000 to 144,000,000 / 250,982,495\n",
      "Predicted rows 144,000,000 to 146,000,000 / 250,982,495\n",
      "Predicted rows 146,000,000 to 148,000,000 / 250,982,495\n",
      "Predicted rows 148,000,000 to 150,000,000 / 250,982,495\n",
      "Predicted rows 150,000,000 to 152,000,000 / 250,982,495\n",
      "Predicted rows 152,000,000 to 154,000,000 / 250,982,495\n",
      "Predicted rows 154,000,000 to 156,000,000 / 250,982,495\n",
      "Predicted rows 156,000,000 to 158,000,000 / 250,982,495\n",
      "Predicted rows 158,000,000 to 160,000,000 / 250,982,495\n",
      "Predicted rows 160,000,000 to 162,000,000 / 250,982,495\n",
      "Predicted rows 162,000,000 to 164,000,000 / 250,982,495\n",
      "Predicted rows 164,000,000 to 166,000,000 / 250,982,495\n",
      "Predicted rows 166,000,000 to 168,000,000 / 250,982,495\n",
      "Predicted rows 168,000,000 to 170,000,000 / 250,982,495\n",
      "Predicted rows 170,000,000 to 172,000,000 / 250,982,495\n",
      "Predicted rows 172,000,000 to 174,000,000 / 250,982,495\n",
      "Predicted rows 174,000,000 to 176,000,000 / 250,982,495\n",
      "Predicted rows 176,000,000 to 178,000,000 / 250,982,495\n",
      "Predicted rows 178,000,000 to 180,000,000 / 250,982,495\n",
      "Predicted rows 180,000,000 to 182,000,000 / 250,982,495\n",
      "Predicted rows 182,000,000 to 184,000,000 / 250,982,495\n",
      "Predicted rows 184,000,000 to 186,000,000 / 250,982,495\n",
      "Predicted rows 186,000,000 to 188,000,000 / 250,982,495\n",
      "Predicted rows 188,000,000 to 190,000,000 / 250,982,495\n",
      "Predicted rows 190,000,000 to 192,000,000 / 250,982,495\n",
      "Predicted rows 192,000,000 to 194,000,000 / 250,982,495\n",
      "Predicted rows 194,000,000 to 196,000,000 / 250,982,495\n",
      "Predicted rows 196,000,000 to 198,000,000 / 250,982,495\n",
      "Predicted rows 198,000,000 to 200,000,000 / 250,982,495\n",
      "Predicted rows 200,000,000 to 202,000,000 / 250,982,495\n",
      "Predicted rows 202,000,000 to 204,000,000 / 250,982,495\n",
      "Predicted rows 204,000,000 to 206,000,000 / 250,982,495\n",
      "Predicted rows 206,000,000 to 208,000,000 / 250,982,495\n",
      "Predicted rows 208,000,000 to 210,000,000 / 250,982,495\n",
      "Predicted rows 210,000,000 to 212,000,000 / 250,982,495\n",
      "Predicted rows 212,000,000 to 214,000,000 / 250,982,495\n",
      "Predicted rows 214,000,000 to 216,000,000 / 250,982,495\n",
      "Predicted rows 216,000,000 to 218,000,000 / 250,982,495\n",
      "Predicted rows 218,000,000 to 220,000,000 / 250,982,495\n",
      "Predicted rows 220,000,000 to 222,000,000 / 250,982,495\n",
      "Predicted rows 222,000,000 to 224,000,000 / 250,982,495\n",
      "Predicted rows 224,000,000 to 226,000,000 / 250,982,495\n",
      "Predicted rows 226,000,000 to 228,000,000 / 250,982,495\n",
      "Predicted rows 228,000,000 to 230,000,000 / 250,982,495\n",
      "Predicted rows 230,000,000 to 232,000,000 / 250,982,495\n",
      "Predicted rows 232,000,000 to 234,000,000 / 250,982,495\n",
      "Predicted rows 234,000,000 to 236,000,000 / 250,982,495\n",
      "Predicted rows 236,000,000 to 238,000,000 / 250,982,495\n",
      "Predicted rows 238,000,000 to 240,000,000 / 250,982,495\n",
      "Predicted rows 240,000,000 to 242,000,000 / 250,982,495\n",
      "Predicted rows 242,000,000 to 244,000,000 / 250,982,495\n",
      "Predicted rows 244,000,000 to 246,000,000 / 250,982,495\n",
      "Predicted rows 246,000,000 to 248,000,000 / 250,982,495\n",
      "Predicted rows 248,000,000 to 250,000,000 / 250,982,495\n",
      "Predicted rows 250,000,000 to 250,982,495 / 250,982,495\n",
      "Final scored rows: 250982495\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 5 – always load saved model and perform inference with progress logging\n",
    "\n",
    "MODEL_PATH = \"../data/outputs/xgb_ranker.model\"\n",
    "\n",
    "print(\"Loading stored model:\", MODEL_PATH)\n",
    "bst = xgb.Booster()\n",
    "bst.load_model(MODEL_PATH)\n",
    "print(\"Model loaded, best iteration:\", bst.best_iteration)\n",
    "\n",
    "FEATURES_PATH = \"../data/outputs/features_week=20200922.parquet\"\n",
    "\n",
    "data = pd.read_parquet(FEATURES_PATH, columns=['customer_id','article_id'] + feature_cols)\n",
    "\n",
    "# enforce correct dtypes before ANY further processing\n",
    "data['customer_id'] = data['customer_id'].astype('int64')\n",
    "data['article_id'] = data['article_id'].astype('int32')\n",
    "\n",
    "# Ensure float32 for XGBoost features\n",
    "for c in feature_cols:\n",
    "    if pd.api.types.is_float_dtype(data[c]):\n",
    "        data[c] = data[c].astype('float32')\n",
    "\n",
    "BATCH = 2_000_000\n",
    "n_rows = len(data)\n",
    "scores = np.empty(n_rows, dtype=np.float32)\n",
    "\n",
    "print(f\"Running inference on {n_rows:,} rows...\")\n",
    "\n",
    "for start in range(0, n_rows, BATCH):\n",
    "    end = min(start + BATCH, n_rows)\n",
    "    dmatrix_batch = xgb.DMatrix(data.iloc[start:end][feature_cols])\n",
    "    # Remove iteration_range to use all trained trees\n",
    "    scores[start:end] = bst.predict(dmatrix_batch, iteration_range=(0, bst.best_iteration + 1))\n",
    "    del dmatrix_batch\n",
    "    gc.collect()\n",
    "    print(f\"Predicted rows {start:,} to {end:,} / {n_rows:,}\")\n",
    "\n",
    "data['score'] = scores\n",
    "del scores\n",
    "gc.collect()\n",
    "\n",
    "print(\"Final scored rows:\", len(data))\n",
    "\n",
    "# 50 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0100f80a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68974f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_240852/2283025980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top12['article_id_str'] = top12['article_id'].astype(str).str.zfill(10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: 1371980\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 – top12 per customer\n",
    "data['customer_id'] = data['customer_id'].astype('int64')\n",
    "\n",
    "data = data.sort_values(['customer_id','score'], ascending=[True,False])\n",
    "top12 = data.groupby('customer_id', group_keys=False).head(12)\n",
    "\n",
    "top12['article_id_str'] = top12['article_id'].astype(str).str.zfill(10)\n",
    "\n",
    "pred_df = (\n",
    "    top12.groupby('customer_id')['article_id_str']\n",
    "         .apply(lambda x: ' '.join(x))\n",
    "         .reset_index()\n",
    "         .rename(columns={'customer_id':'customer_id_int',\n",
    "                          'article_id_str':'prediction'})\n",
    ")\n",
    "\n",
    "pred_df['customer_id_int'] = pred_df['customer_id_int'].astype('int64')\n",
    "\n",
    "del top12, data\n",
    "gc.collect()\n",
    "\n",
    "print(\"Predictions:\", len(pred_df))\n",
    "\n",
    "# 8 min 20s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e66f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions matched: 1371980\n",
      "Rows after merge: 1371980\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 – merge with sample + fallback\n",
    "\n",
    "# Must match preprocessing pipeline\n",
    "def hex16_to_int(s):\n",
    "    return np.int64(np.uint64(int(s[-16:], 16)))\n",
    "\n",
    "sample = pd.read_csv('../data/input_data/sample_submission.csv')\n",
    "\n",
    "# Convert customer_id using identical hashing\n",
    "sample['customer_id_int'] = sample['customer_id'].apply(hex16_to_int)\n",
    "\n",
    "# Drop any existing prediction column (same as LGBM)\n",
    "sample = sample.drop(columns=['prediction'], errors='ignore')\n",
    "\n",
    "# pred_df already contains correct int64 customer_id from features\n",
    "pred_df['customer_id_int'] = pred_df['customer_id_int'].astype('int64')\n",
    "\n",
    "# Merge exactly like LGBM\n",
    "sub = sample.merge(pred_df, how='left', on='customer_id_int')\n",
    "\n",
    "# Diagnostics\n",
    "print(\"Predictions matched:\", sub['prediction'].notna().sum())\n",
    "print(\"Rows after merge:\", len(sub))\n",
    "\n",
    "del sample\n",
    "gc.collect()\n",
    "\n",
    "gp = json.load(open('../data/outputs/general_pred_str.json'))\n",
    "fallback_str = gp['general_pred_str']\n",
    "fallback_items = fallback_str.split()\n",
    "\n",
    "sub['prediction'] = sub['prediction'].fillna(fallback_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1468ce2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/submission/xgboost_ranker_submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_id_int</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>6883939031699146327</td>\n",
       "      <td>0568601043 0568601044 0568601006 0568601007 09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>-7200416642310594310</td>\n",
       "      <td>0673677002 0918522001 0706016001 0158340001 09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>-6846340800584936</td>\n",
       "      <td>0794321007 0794321011 0794321008 0918292001 08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n",
       "      <td>-94071612138601410</td>\n",
       "      <td>0794321011 0730683050 0804992017 0805000001 07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n",
       "      <td>-283965518499174310</td>\n",
       "      <td>0896152002 0818320001 0730683062 0730683050 07...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id      customer_id_int  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...  6883939031699146327   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e... -7200416642310594310   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...    -6846340800584936   \n",
       "3  00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...   -94071612138601410   \n",
       "4  00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...  -283965518499174310   \n",
       "\n",
       "                                          prediction  \n",
       "0  0568601043 0568601044 0568601006 0568601007 09...  \n",
       "1  0673677002 0918522001 0706016001 0158340001 09...  \n",
       "2  0794321007 0794321011 0794321008 0918292001 08...  \n",
       "3  0794321011 0730683050 0804992017 0805000001 07...  \n",
       "4  0896152002 0818320001 0730683062 0730683050 07...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 8 – pad/trim to 12\n",
    "def pad_to_12(pred):\n",
    "    items = pred.split()\n",
    "    if len(items) >= 12:\n",
    "        return ' '.join(items[:12])\n",
    "    used = set(items)\n",
    "    for art in fallback_items:\n",
    "        if art not in used:\n",
    "            items.append(art)\n",
    "            used.add(art)\n",
    "        if len(items) == 12:\n",
    "            break\n",
    "    return ' '.join(items)\n",
    "\n",
    "sub['prediction'] = sub['prediction'].apply(pad_to_12)\n",
    "\n",
    "out_path = '../data/submission/xgboost_ranker_submission.csv'\n",
    "sub[['customer_id','prediction']].to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "sub.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f01f1e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_code_match 136.48648071289062\n",
      "value 110.96809387207031\n",
      "window_type_code 65.45519256591797\n",
      "garment_group_no 26.101293563842773\n",
      "days_since_last_purchase 23.271060943603516\n",
      "cust_purchases_1w 17.962879180908203\n",
      "index_group_no 15.541091918945312\n",
      "product_group_name 15.514805793762207\n",
      "article_total_purchases 14.582350730895996\n",
      "index_group_match 13.872264862060547\n",
      "section_no 12.262479782104492\n",
      "product_type_no 12.199393272399902\n",
      "department_no 11.977296829223633\n",
      "article_unique_customers 11.367769241333008\n",
      "age_bucket 10.799942970275879\n",
      "product_code 10.514240264892578\n",
      "colour_group_code 10.3543701171875\n",
      "article_mean_price 10.264841079711914\n",
      "graphical_appearance_no 10.191529273986816\n",
      "index_code 9.099257469177246\n",
      "age 8.88410758972168\n",
      "customer_mean_price 8.69286823272705\n",
      "customer_days_since_last_purchase 8.271799087524414\n",
      "customer_unique_articles 8.256797790527344\n",
      "article_mean_age 7.945797443389893\n",
      "perceived_colour_value_id 7.407469749450684\n",
      "cust_purchases_4w 7.09806489944458\n",
      "perceived_colour_master_id 6.719933032989502\n",
      "age_sensitivity 6.578423500061035\n",
      "price_sensitivity 6.574829578399658\n",
      "customer_total_purchases 6.414450168609619\n",
      "club_member_status 5.172845840454102\n",
      "fashion_news_frequency 4.714519500732422\n",
      "postal_code 4.005802154541016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 9 – feature importance\n",
    "imp = bst.get_score(importance_type='gain')\n",
    "\n",
    "name_map = {f\"f{i}\": name for i,name in enumerate(feature_cols)}\n",
    "items = [(name_map.get(k,k), v) for k,v in imp.items()]\n",
    "\n",
    "for n,v in sorted(items, key=lambda x: -x[1]):\n",
    "    print(n, v)\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58093600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Offline MAP@12 (Validation Week 20200916): 0.013480488027471373\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 - OPTIONAL – Compute offline MAP@12 on 2020-09-16 validation set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Extract ground truth for validation week\n",
    "# valid_df already contains labels for week 20200916 after filtering groups\n",
    "gt = valid_df[valid_df['label'] == 1][['customer_id', 'article_id']].copy()\n",
    "gt['article_id_str'] = gt['article_id'].astype(str).str.zfill(10)\n",
    "\n",
    "# Map each customer → set of purchased items\n",
    "gt_dict = gt.groupby('customer_id')['article_id_str'].apply(set).to_dict()\n",
    "\n",
    "# 2. Predictions (after your top-12 grouping)\n",
    "# Should be: customer_id_int, prediction (10-digit strings)\n",
    "pred = pred_df.copy()\n",
    "pred['pred_list'] = pred['prediction'].str.split()\n",
    "\n",
    "# Convert ID type if needed\n",
    "pred['customer_id'] = pred['customer_id_int'].astype('int64')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Compute MAP@12\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def apk(actual_set, predicted_list, k=12):\n",
    "    \"\"\"Average Precision@k for a single user.\"\"\"\n",
    "    if len(predicted_list) > k:\n",
    "        predicted_list = predicted_list[:k]\n",
    "    score = 0.0\n",
    "    hits = 0\n",
    "    for i, p in enumerate(predicted_list, start=1):\n",
    "        if p in actual_set:\n",
    "            hits += 1\n",
    "            score += hits / i\n",
    "    if not actual_set:\n",
    "        return 0.0\n",
    "    return score / min(len(actual_set), k)\n",
    "\n",
    "# Apply APK@12 for each customer\n",
    "scores = []\n",
    "for cid, row in pred.iterrows():\n",
    "    user = row['customer_id']\n",
    "    actual = gt_dict.get(user, set())\n",
    "    pred_list = row['pred_list']\n",
    "    scores.append(apk(actual, pred_list, k=12))\n",
    "\n",
    "map12 = np.mean(scores)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"Offline MAP@12 (Validation Week 20200916):\", map12)\n",
    "print(\"=====================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddb3d41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
