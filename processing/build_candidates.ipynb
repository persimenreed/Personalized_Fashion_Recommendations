{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d5626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 – imports\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2eda6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>value</th>\n",
       "      <th>window_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9223352921020755230</td>\n",
       "      <td>673396002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>biweekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9223352921020755230</td>\n",
       "      <td>812167004</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>biweekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9223352921020755230</td>\n",
       "      <td>849493006</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>older</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9223352921020755230</td>\n",
       "      <td>706016001</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>older</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9223352921020755230</td>\n",
       "      <td>568597006</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>older</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           customer_id  article_id     value window_type\n",
       "0 -9223352921020755230   673396002  1.000000    biweekly\n",
       "1 -9223352921020755230   812167004  0.500000    biweekly\n",
       "2 -9223352921020755230   849493006  0.333333       older\n",
       "3 -9223352921020755230   706016001  0.250000       older\n",
       "4 -9223352921020755230   568597006  0.200000       older"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2 – load all candidate files\n",
    "\n",
    "candidate_files = [\n",
    "    '../data/outputs/candidates_weekly_trending.parquet',\n",
    "    '../data/outputs/candidates_popularity.parquet',\n",
    "    '../data/outputs/candidates_itemcf.parquet',\n",
    "    '../data/outputs/candidates_recent_top.parquet',\n",
    "    '../data/outputs/candidates_repurchase.parquet',\n",
    "    '../data/outputs/candidates_user_overlap.parquet',\n",
    "    '../data/outputs/candidates_age_bucket_pop.parquet',\n",
    "    '../data/outputs/candidates_category_affinity.parquet',\n",
    "    '../data/outputs/candidates_same_product.parquet',\n",
    "    '../data/outputs/candidates_embedding.parquet',\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for path in candidate_files:\n",
    "    if path.endswith('.parquet'):\n",
    "        df = pd.read_parquet(path)\n",
    "        # FIX 1: Explicitly keep only core columns to drop 'age_bucket', 'customer_id_hex', etc.\n",
    "        cols = ['customer_id', 'article_id', 'value', 'window_type']\n",
    "        df = df[[c for c in cols if c in df.columns]]\n",
    "    else:\n",
    "        df = pd.read_csv(\n",
    "            path,\n",
    "            usecols=['customer_id', 'article_id', 'value', 'window_type'],\n",
    "            dtype={\n",
    "                'customer_id': 'int64',\n",
    "                'article_id': 'int32',\n",
    "                'value': 'float32',\n",
    "                'window_type': 'category',\n",
    "            },\n",
    "        )\n",
    "    \n",
    "    # Enforce types immediately\n",
    "    df['article_id'] = df['article_id'].astype('int32')\n",
    "    df['customer_id'] = df['customer_id'].astype('int64')\n",
    "    df['value'] = df['value'].astype('float32')\n",
    "    if 'window_type' in df.columns:\n",
    "        df['window_type'] = df['window_type'].astype('category')\n",
    "\n",
    "    # --- NORMALIZE VALUES (Reciprocal Rank) ---\n",
    "    # Sort descending to identify top items per user\n",
    "    # Note: For sources with constant value (like category_affinity), this relies on the input file order,\n",
    "    # which is usually sorted by relevance anyway.\n",
    "    df.sort_values(['customer_id', 'value'], ascending=[True, False], inplace=True)\n",
    "    \n",
    "    # Compute rank (1-based)\n",
    "    df['rank'] = df.groupby('customer_id').cumcount() + 1\n",
    "    \n",
    "    # Reciprocal rank: 1st=1.0, 2nd=0.5, 3rd=0.33...\n",
    "    # This standardizes \"importance\" across all sources regardless of raw score scale.\n",
    "    df['value'] = (1.0 / df['rank']).astype('float32')\n",
    "    \n",
    "    # Drop rank column to save memory\n",
    "    df.drop(columns=['rank'], inplace=True)\n",
    "        \n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all candidates\n",
    "all_cand = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "all_cand.head()\n",
    "\n",
    "# 1 min 56 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588db00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/outputs/candidates_weekly_trending.parquet\n",
      "  rows: 7851328\n",
      "  unique customers: 959265\n",
      "  mean cand / customer: 8.184733102948611\n",
      "../data/outputs/candidates_popularity.parquet\n",
      "  rows: 32927520\n",
      "  unique customers: 1371980\n",
      "  mean cand / customer: 24.0\n",
      "../data/outputs/candidates_itemcf.parquet\n",
      "  rows: 39487109\n",
      "  unique customers: 319144\n",
      "  mean cand / customer: 123.72818852931591\n",
      "../data/outputs/candidates_recent_top.parquet\n",
      "  rows: 9653601\n",
      "  unique customers: 867957\n",
      "  mean cand / customer: 11.122211123362103\n",
      "../data/outputs/candidates_repurchase.parquet\n",
      "  rows: 18409081\n",
      "  unique customers: 1356709\n",
      "  mean cand / customer: 13.568923770683323\n",
      "../data/outputs/candidates_user_overlap.parquet\n",
      "  rows: 18442629\n",
      "  unique customers: 279058\n",
      "  mean cand / customer: 66.08887399752022\n",
      "../data/outputs/candidates_age_bucket_pop.parquet\n",
      "  rows: 137198000\n",
      "  unique customers: 1371980\n",
      "  mean cand / customer: 100.0\n",
      "../data/outputs/candidates_category_affinity.parquet\n",
      "  rows: 27245620\n",
      "  unique customers: 1362281\n",
      "  mean cand / customer: 20.0\n",
      "../data/outputs/candidates_same_product.parquet\n",
      "  rows: 13259768\n",
      "  unique customers: 728475\n",
      "  mean cand / customer: 18.202090668863036\n",
      "../data/outputs/candidates_embedding.parquet\n",
      "  rows: 53400600\n",
      "  unique customers: 534006\n",
      "  mean cand / customer: 100.0\n"
     ]
    }
   ],
   "source": [
    "# New Cell – debug per-source coverage\n",
    "for path, df in zip(candidate_files, dfs):\n",
    "    print(path)\n",
    "    print(\"  rows:\", len(df))\n",
    "    print(\"  unique customers:\", df['customer_id'].nunique())\n",
    "    print(\"  mean cand / customer:\", df.groupby('customer_id')['article_id'].nunique().mean())\n",
    "\n",
    "# 1 min 26 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4df6481f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>value</th>\n",
       "      <th>window_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-928273397339854563</td>\n",
       "      <td>909370001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>age_bucket_pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4686014925307215657</td>\n",
       "      <td>630888001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>repurchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9223273623650064010</td>\n",
       "      <td>433414040</td>\n",
       "      <td>1.0</td>\n",
       "      <td>older</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685955727631018272</td>\n",
       "      <td>817150004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>repurchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4686048493530892581</td>\n",
       "      <td>823791002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>repurchase</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           customer_id  article_id  value     window_type\n",
       "0  -928273397339854563   909370001    1.0  age_bucket_pop\n",
       "1  4686014925307215657   630888001    1.0      repurchase\n",
       "2 -9223273623650064010   433414040    1.0           older\n",
       "3  4685955727631018272   817150004    1.0      repurchase\n",
       "4  4686048493530892581   823791002    1.0      repurchase"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3 – deduplicate per (customer_id, article_id), keep max value\n",
    "# OPTIMIZATION: Use inplace operations to avoid memory spikes from copying\n",
    "all_cand.sort_values('value', ascending=False, inplace=True)\n",
    "all_cand.drop_duplicates(['customer_id', 'article_id'], keep='first', inplace=True)\n",
    "all_cand.reset_index(drop=True, inplace=True)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Ensure dtypes (safety)\n",
    "all_cand['customer_id'] = all_cand['customer_id'].astype('int64')\n",
    "all_cand['article_id'] = all_cand['article_id'].astype('int32')\n",
    "all_cand['value'] = all_cand['value'].astype('float32')\n",
    "all_cand.head()\n",
    "\n",
    "# 23 gb, 4 min 34 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b74b58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Recall check vs last 7 days labels]\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mgc\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[Recall check vs last 7 days labels]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tx = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/input_data/transactions_train.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mt_dat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcustomer_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43marticle_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mt_dat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstring\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcustomer_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstring\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43marticle_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mint32\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m tx[\u001b[33m'\u001b[39m\u001b[33mcustomer_id_int\u001b[39m\u001b[33m'\u001b[39m] = tx[\u001b[33m'\u001b[39m\u001b[33mcustomer_id\u001b[39m\u001b[33m'\u001b[39m].str[-\u001b[32m16\u001b[39m:].apply(\u001b[38;5;28;01mlambda\u001b[39;00m h: np.int64(np.uint64(\u001b[38;5;28mint\u001b[39m(h,\u001b[32m16\u001b[39m))))\n\u001b[32m     11\u001b[39m tx[\u001b[33m'\u001b[39m\u001b[33mt_dat\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(tx[\u001b[33m'\u001b[39m\u001b[33mt_dat\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ICTProject/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ICTProject/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ICTProject/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ICTProject/.venv/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "# ONLY RUN TO CHECK COVER. SCRIPT TAKES ~10 MINUTES ===\n",
    "import pandas as pd, numpy as np, gc\n",
    "\n",
    "print(\"\\n[Recall check vs last 7 days labels]\")\n",
    "tx = pd.read_csv(\n",
    "    '../data/input_data/transactions_train.csv',\n",
    "    usecols=['t_dat','customer_id','article_id'],\n",
    "    dtype={'t_dat':'string','customer_id':'string','article_id':'int32'}\n",
    ")\n",
    "tx['customer_id_int'] = tx['customer_id'].str[-16:].apply(lambda h: np.int64(np.uint64(int(h,16))))\n",
    "tx['t_dat'] = pd.to_datetime(tx['t_dat'])\n",
    "last_ts = tx['t_dat'].max()\n",
    "cut_ts  = last_ts - pd.Timedelta(days=7)\n",
    "\n",
    "label_tx = tx[(tx['t_dat'] > cut_ts) & (tx['t_dat'] <= last_ts)]\n",
    "labels = (label_tx.groupby(['customer_id_int','article_id'])['t_dat']\n",
    "          .size().reset_index()[['customer_id_int','article_id']])\n",
    "labels = labels.rename(columns={'customer_id_int':'customer_id'})\n",
    "\n",
    "# Build set for quick per-source coverage\n",
    "label_set = set(map(tuple, labels[['customer_id','article_id']].to_numpy()))\n",
    "\n",
    "# Overall coverage using deduped pool\n",
    "cand_pairs = all_cand[['customer_id','article_id']].drop_duplicates()\n",
    "covered = labels.merge(cand_pairs, on=['customer_id','article_id'], how='left', indicator=True)\n",
    "overall = (covered['_merge'] == 'both').sum()\n",
    "print(f\"Total label pairs: {len(labels)}\")\n",
    "print(f\"Covered by all candidates: {overall}\")\n",
    "print(f\"Recall: {overall/len(labels):.4f}\")\n",
    "\n",
    "# Per-source contribution (approximate coverage)\n",
    "for path, df in zip(candidate_files, dfs):\n",
    "    pairs = set(map(tuple, df[['customer_id','article_id']].drop_duplicates().to_numpy()))\n",
    "    inter = len(label_set & pairs)\n",
    "    print(f\"{path:55s} covers {inter:7d} ({inter/len(labels):.4f})\")\n",
    "\n",
    "del tx, label_tx, labels, cand_pairs, covered, label_set, pairs, dfs, df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c370b1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers missing candidates: 0\n"
     ]
    }
   ],
   "source": [
    "# Add synthetic candidates for submission customers missing from candidate pool\n",
    "sub = pd.read_csv('../data/input_data/sample_submission.csv', usecols=['customer_id'], dtype={'customer_id': 'string'})\n",
    "sub['customer_id'] = sub['customer_id'].str[-16:].apply(lambda h: np.int64(np.uint64(int(h, 16))))\n",
    "\n",
    "missing = set(sub['customer_id'].unique()) - set(all_cand['customer_id'].unique())\n",
    "print(\"Customers missing candidates:\", len(missing))\n",
    "\n",
    "if missing:\n",
    "    gp = pd.read_json('../data/outputs/general_pred_str.json', typ='series')\n",
    "    top_items = [int(x) for x in gp['general_pred_str'].split()][:20]  # small cap for features\n",
    "    synth = pd.DataFrame(\n",
    "        [(cid, art, 0.001, 'synthetic') for cid in missing for art in top_items],\n",
    "        columns=['customer_id', 'article_id', 'value', 'window_type']\n",
    "    )\n",
    "    synth['customer_id'] = synth['customer_id'].astype('int64')\n",
    "    synth['article_id']  = synth['article_id'].astype('int32')\n",
    "    synth['value']       = synth['value'].astype('float32')\n",
    "    all_cand = pd.concat([all_cand, synth], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa52432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of customers: 1371980\n",
      "Mean candidates per customer: 191.26813947725185\n",
      "Median candidates per customer: 128.0\n",
      "Customer with max candidates: 768\n",
      "Customer with min candidates: 100\n"
     ]
    }
   ],
   "source": [
    "cand_per_cust = all_cand.groupby('customer_id')['article_id'].nunique()\n",
    "print(\"Number of customers:\", cand_per_cust.shape[0])\n",
    "print(\"Mean candidates per customer:\", cand_per_cust.mean())\n",
    "print(\"Median candidates per customer:\", cand_per_cust.median())\n",
    "print(\"Customer with max candidates:\", cand_per_cust.max())\n",
    "print(\"Customer with min candidates:\", cand_per_cust.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdcfc0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4 – save merged candidates_for_ranker.csv\n",
    "\n",
    "all_cand.to_parquet('../data/outputs/candidates.parquet', index=False)\n",
    "\n",
    "del all_cand\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aba25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab573530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b56a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6252a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
