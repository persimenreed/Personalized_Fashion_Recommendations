{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ccc3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 – config & imports\n",
    "import gc, json, numpy as np, pandas as pd, os, hashlib\n",
    "\n",
    "FEATURE_PATH = \"../data/outputs/features.parquet\"\n",
    "TX_PATH = \"../data/input_data/transactions_train.csv\"\n",
    "FEATURE_COLS_JSON = \"../data/outputs/feature_cols.json\"\n",
    "\n",
    "LABEL_LOOKBACK_DAYS = 7\n",
    "NEG_SAMPLE_CLF = 1_000_000\n",
    "RANDOM_SEED = 42\n",
    "RANK_TOP_K_EVAL = 12\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "def hex16_to_int(s):\n",
    "    return np.int64(np.uint64(int(s[-16:],16)))\n",
    "\n",
    "# Deterministic split function (modulus)\n",
    "def is_valid_customer(cid: int) -> bool:\n",
    "    return (cid % 5) == 0\n",
    "\n",
    "print(\"Config loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e4f317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (262416062, 34)\n",
      "Loaded columns: 34\n",
      "Number of feature_cols (including ids): 35\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 – load base features (already engineered) – memory safe\n",
    "# Load feature column order first (it already includes customer_id, article_id)\n",
    "if os.path.exists(FEATURE_COLS_JSON):\n",
    "    with open(FEATURE_COLS_JSON) as f:\n",
    "        feature_cols = json.load(f)[\"feature_cols\"]\n",
    "else:\n",
    "    # Fallback: read whole file once to derive columns\n",
    "    tmp_all = pd.read_parquet(FEATURE_PATH)\n",
    "    feature_cols = ['customer_id','article_id'] + [c for c in tmp_all.columns if c not in ['customer_id','article_id']]\n",
    "    del tmp_all\n",
    "    gc.collect()\n",
    "\n",
    "# Subset to only columns needed for modeling (exclude anything you no longer use)\n",
    "# Ensure id columns always present\n",
    "core_cols = ['customer_id','article_id']\n",
    "# Keep a minimal set of numeric / categorical predictors (adjust as needed)\n",
    "predictor_subset = [\n",
    "    'value','window_type_code',\n",
    "    'customer_total_purchases','customer_unique_articles',\n",
    "    'article_total_purchases','article_unique_customers',\n",
    "    'cust_purchases_1w','cust_purchases_4w',\n",
    "    'days_since_last_purchase','customer_days_since_last_purchase',\n",
    "    'age','club_member_status','fashion_news_frequency','postal_code',\n",
    "    'product_type_no','product_group_name','index_code','section_no',\n",
    "    'graphical_appearance_no','colour_group_code','perceived_colour_value_id',\n",
    "    'perceived_colour_master_id','index_group_no','garment_group_no',\n",
    "    'article_mean_price','customer_mean_price', \n",
    "    'price_sensitivity', 'index_group_match',\n",
    "    'age_sensitivity', 'article_mean_age','department_no', 'product_code_match'\n",
    "]\n",
    "\n",
    "\n",
    "needed_cols = [c for c in feature_cols if c in (core_cols + predictor_subset)]\n",
    "\n",
    "# Read only required columns\n",
    "features = pd.read_parquet(FEATURE_PATH, columns=needed_cols)\n",
    "\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Loaded columns:\", len(needed_cols))\n",
    "\n",
    "# Types\n",
    "features['customer_id'] = features['customer_id'].astype('int64')\n",
    "features['article_id']  = features['article_id'].astype('int32')\n",
    "\n",
    "print(\"Number of feature_cols (including ids):\", len(feature_cols))\n",
    "\n",
    "# ~ 30gb of ram usage, 11s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f60e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive label rows: 213728\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 – build labels (last LABEL_LOOKBACK_DAYS) – optimized\n",
    "tx = pd.read_csv(\n",
    "    TX_PATH,\n",
    "    usecols=['t_dat','customer_id','article_id'],\n",
    "    dtype={'t_dat':'string','customer_id':'string','article_id':'int32'},\n",
    "    parse_dates=['t_dat']  # parse directly\n",
    ")\n",
    "tx['customer_id'] = tx['customer_id'].str[-16:].apply(hex16_to_int)\n",
    "\n",
    "last_ts = tx['t_dat'].max()\n",
    "cut_ts  = last_ts - pd.Timedelta(days=LABEL_LOOKBACK_DAYS)\n",
    "\n",
    "label_tx = tx[(tx['t_dat'] > cut_ts) & (tx['t_dat'] <= last_ts)][['customer_id','article_id']].drop_duplicates()\n",
    "labels = label_tx.assign(label=1)\n",
    "del label_tx, tx\n",
    "gc.collect()\n",
    "print(\"Positive label rows:\", len(labels))\n",
    "\n",
    "# ~ 24gb of ram usage, 3 min 47s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "544380a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset rows: 262416062 Pos covered: 47462 Total pos labels: 213728 Recall: 0.2221\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 – merge labels (unchanged logic, ensure categories preserved)\n",
    "data = features.merge(labels, on=['customer_id','article_id'], how='left')\n",
    "data['label'] = data['label'].fillna(0).astype('int8')\n",
    "\n",
    "# Compute coverage/recall before freeing labels\n",
    "total_pos_labels = int(len(labels))\n",
    "pos_covered = int(data['label'].sum())\n",
    "recall = (pos_covered / total_pos_labels) if total_pos_labels else 0.0\n",
    "\n",
    "del features, labels\n",
    "gc.collect()\n",
    "print(\n",
    "    \"Merged dataset rows:\", len(data),\n",
    "    \"Pos covered:\", pos_covered,\n",
    "    \"Total pos labels:\", total_pos_labels,\n",
    "    f\"Recall: {recall:.4f}\"\n",
    ")\n",
    "\n",
    "# ~ 50gb of ram usage, 2 min 46s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bab8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique customers: 1371980\n",
      "Train rows (ranking): 210021247 Valid rows (ranking): 52394815\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 – deterministic split (vectorized, no apply)\n",
    "cust_ids = data['customer_id'].unique()\n",
    "print(\"Unique customers:\", len(cust_ids))\n",
    "\n",
    "valid_mask = (data['customer_id'] % 5) == 0\n",
    "train_mask = ~valid_mask\n",
    "\n",
    "# No .copy() unless you mutate\n",
    "train_full = data.loc[train_mask]\n",
    "valid_full = data.loc[valid_mask]\n",
    "\n",
    "print(\"Train rows (ranking):\", len(train_full), \"Valid rows (ranking):\", len(valid_full))\n",
    "\n",
    "# ~ 31gb of ram usage, 21s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60541c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows before downsampling: 210021247\n",
      "Downsampling negatives from 209983277 to 1000000...\n",
      "Train rows after downsampling: 1037970\n",
      "Saved ranking data + group arrays.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6 – ranking groups with NEGATIVE DOWNSAMPLING (Winning Solution Approach)\n",
    "\n",
    "print(f\"Train rows before downsampling: {len(train_full)}\")\n",
    "\n",
    "# 1. Split Positive and Negative\n",
    "pos_train = train_full[train_full['label'] == 1]\n",
    "neg_train = train_full[train_full['label'] == 0]\n",
    "\n",
    "# 2. Downsample Negatives\n",
    "# The winning solution suggests ~1M negatives. NEG_SAMPLE_CLF is set to 1M in config.\n",
    "if len(neg_train) > NEG_SAMPLE_CLF:\n",
    "    print(f\"Downsampling negatives from {len(neg_train)} to {NEG_SAMPLE_CLF}...\")\n",
    "    neg_train = neg_train.sample(n=NEG_SAMPLE_CLF, random_state=RANDOM_SEED)\n",
    "\n",
    "# 3. Recombine\n",
    "train_full = pd.concat([pos_train, neg_train], ignore_index=True)\n",
    "print(f\"Train rows after downsampling: {len(train_full)}\")\n",
    "\n",
    "# 4. Sort by customer_id (REQUIRED for LightGBM Ranker groups)\n",
    "# LightGBM requires all rows for a specific query (customer) to be contiguous.\n",
    "train_full = train_full.sort_values('customer_id')\n",
    "valid_full = valid_full.sort_values('customer_id')\n",
    "\n",
    "# 5. Calculate Group Sizes\n",
    "# value_counts(sort=False).sort_index() aligns with the sorted dataframe\n",
    "train_group_sizes = train_full['customer_id'].value_counts(sort=False).sort_index().to_numpy()\n",
    "valid_group_sizes = valid_full['customer_id'].value_counts(sort=False).sort_index().to_numpy()\n",
    "\n",
    "# 6. Save\n",
    "np.save(\"../data/outputs/groups_train.npy\", train_group_sizes)\n",
    "np.save(\"../data/outputs/groups_valid.npy\", valid_group_sizes)\n",
    "\n",
    "train_full.to_parquet(\"../data/outputs/train_rank.parquet\", index=False)\n",
    "valid_full.to_parquet(\"../data/outputs/valid_rank.parquet\", index=False)\n",
    "print(\"Saved ranking data + group arrays.\")\n",
    "\n",
    "# Clean up temp vars\n",
    "del pos_train, neg_train\n",
    "gc.collect()\n",
    "\n",
    "# ~ 45gb of ram usage, 6 min 12s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a672c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 – meta (add memory info)\n",
    "model_features = [c for c in feature_cols if c not in ['customer_id','article_id','label'] and c in data.columns]\n",
    "\n",
    "with open(\"../data/outputs/dataset_meta.json\",\"w\") as f:\n",
    "    json.dump({\n",
    "        \"last_ts\": str(last_ts),\n",
    "        \"cut_ts\": str(cut_ts),\n",
    "        \"label_lookback_days\": LABEL_LOOKBACK_DAYS,\n",
    "        \"neg_sample_clf\": NEG_SAMPLE_CLF,\n",
    "        \"feature_cols_full\": feature_cols,\n",
    "        \"model_features\": model_features,\n",
    "        \"rank_train_rows\": len(train_full),\n",
    "        \"rank_valid_rows\": len(valid_full),\n",
    "        \"rank_at\": RANK_TOP_K_EVAL\n",
    "    }, f)\n",
    "\n",
    "print(\"Meta saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44786e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup done.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 – cleanup\n",
    "del data, train_full, valid_full\n",
    "gc.collect()\n",
    "print(\"Cleanup done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e604b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e486b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
