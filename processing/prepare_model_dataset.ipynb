{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ccc3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 – config & imports\n",
    "import gc, json, numpy as np, pandas as pd, os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "TX_PATH = \"../data/input_data/transactions_train.csv\"\n",
    "FEATURE_DIR = \"../data/outputs\"\n",
    "\n",
    "# weeks we have built features for\n",
    "TRAIN_WEEKS = [20200819, 20200826,20200902, 20200909]   # train label weeks\n",
    "VALID_WEEK  = 20200916               # validation label week\n",
    "\n",
    "LABEL_LOOKBACK_DAYS = 7\n",
    "RANDOM_SEED = 42\n",
    "RANK_TOP_K_EVAL = 12\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "def hex16_to_int(s):\n",
    "    return np.int64(np.uint64(int(s[-16:],16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e4f317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available features: 36\n",
      "Model feature count: 34\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 – load feature column order (saved from features pipeline)\n",
    "with open(f\"{FEATURE_DIR}/feature_cols.json\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "FULL_FEATURE_COLS = meta[\"feature_cols\"]\n",
    "\n",
    "# ID columns always needed\n",
    "ID_COLS = [\"customer_id\", \"article_id\"]\n",
    "\n",
    "print(\"Total available features:\", len(FULL_FEATURE_COLS))\n",
    "\n",
    "# For now, use all non-ID columns as model features\n",
    "MODEL_FEATURES = [c for c in FULL_FEATURE_COLS if c not in ID_COLS]\n",
    "print(\"Model feature count:\", len(MODEL_FEATURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8c5753a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating stream file from: ../data/outputs/features_week=20200819.parquet\n",
      "Saved streamed: ../data/outputs/features_stream_week=20200819.parquet\n",
      "Creating stream file from: ../data/outputs/features_week=20200826.parquet\n",
      "Saved streamed: ../data/outputs/features_stream_week=20200826.parquet\n",
      "Removing old stream file: ../data/outputs/features_stream_week=20200916.parquet\n",
      "Creating stream file from: ../data/outputs/features_week=20200916.parquet\n",
      "Saved streamed: ../data/outputs/features_stream_week=20200916.parquet\n",
      "Train stream files: ['../data/outputs/features_stream_week=20200819.parquet', '../data/outputs/features_stream_week=20200826.parquet']\n",
      "Valid stream file: ../data/outputs/features_stream_week=20200916.parquet\n"
     ]
    }
   ],
   "source": [
    "# # Cell 3 – stream-load each week's features to a normalized file\n",
    "# train_files = []\n",
    "# valid_file = None\n",
    "\n",
    "# for w in TRAIN_WEEKS + [VALID_WEEK]:\n",
    "#     out_path = f\"../data/outputs/features_stream_week={w}.parquet\"\n",
    "\n",
    "#     # MODIFIED: Always overwrite, never skip\n",
    "#     if os.path.exists(out_path):\n",
    "#         print(f\"Removing old stream file: {out_path}\")\n",
    "#         os.remove(out_path)\n",
    "\n",
    "#     file_path = f\"{FEATURE_DIR}/features_week={w}.parquet\"\n",
    "#     print(\"Creating stream file from:\", file_path)\n",
    "\n",
    "#     # Load only required columns\n",
    "#     df = pd.read_parquet(file_path, columns=ID_COLS + MODEL_FEATURES)\n",
    "#     df[\"week_end\"] = np.int32(w)\n",
    "\n",
    "#     # Save normalized version\n",
    "#     df.to_parquet(out_path, index=False)\n",
    "#     print(\"Saved streamed:\", out_path)\n",
    "\n",
    "#     del df\n",
    "#     gc.collect()\n",
    "\n",
    "#     if w == VALID_WEEK:\n",
    "#         valid_file = out_path\n",
    "#     else:\n",
    "#         train_files.append(out_path)\n",
    "\n",
    "# print(\"Train stream files:\", train_files)\n",
    "# print(\"Valid stream file:\", valid_file)\n",
    "\n",
    "# # 11 min, 22s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6e2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 – build labels and merge into train/valid (FULLY STREAMING)\n",
    "\n",
    "import shutil\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "TX_CHUNK_ROWS = 5_000_000\n",
    "LABELS_DIR = \"../data/outputs/labels_temp\"\n",
    "\n",
    "def labels_path_for_week(w):\n",
    "    return f\"{LABELS_DIR}/labels_week={w}.parquet\"\n",
    "\n",
    "def empty_labels_df():\n",
    "    return pd.DataFrame({\n",
    "        'customer_id': pd.Series(dtype='int64'),\n",
    "        'article_id': pd.Series(dtype='int32'),\n",
    "        'label': pd.Series(dtype='int8'),\n",
    "        'week_end': pd.Series(dtype='int32'),\n",
    "    })\n",
    "\n",
    "def infer_week_from_features_path(p):\n",
    "    return int(p.split('week=')[1].split('.')[0])\n",
    "\n",
    "def load_labels_for_week(w):\n",
    "    path = labels_path_for_week(w)\n",
    "    if os.path.exists(path):\n",
    "        return pd.read_parquet(path)\n",
    "    return empty_labels_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f23994ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming transactions to build labels...\n",
      "Total positive labels (all weeks): 1150836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.1 – Build labels FIRST in streaming mode\n",
    "print(\"Streaming transactions to build labels...\")\n",
    "week_windows = {\n",
    "    w: {\n",
    "        \"cut_ts\": pd.to_datetime(str(w)) - pd.Timedelta(days=LABEL_LOOKBACK_DAYS),\n",
    "        \"last_ts\": pd.to_datetime(str(w)),\n",
    "    }\n",
    "    for w in TRAIN_WEEKS + [VALID_WEEK]\n",
    "}\n",
    "\n",
    "shutil.rmtree(LABELS_DIR, ignore_errors=True)\n",
    "os.makedirs(LABELS_DIR, exist_ok=True)\n",
    "\n",
    "label_paths = {w: labels_path_for_week(w) for w in week_windows}\n",
    "label_writers = {}\n",
    "\n",
    "tx_iter = pd.read_csv(\n",
    "    TX_PATH,\n",
    "    usecols=[\"t_dat\", \"customer_id\", \"article_id\"],\n",
    "    dtype={\"t_dat\": \"string\", \"customer_id\": \"string\", \"article_id\": \"int32\"},\n",
    "    chunksize=TX_CHUNK_ROWS,\n",
    ")\n",
    "\n",
    "for idx, chunk in enumerate(tx_iter, start=1):\n",
    "    chunk['t_dat'] = pd.to_datetime(chunk['t_dat'])\n",
    "    chunk[\"customer_id\"] = chunk[\"customer_id\"].str[-16:].apply(hex16_to_int)\n",
    "\n",
    "    for w, bounds in week_windows.items():\n",
    "        mask = (chunk[\"t_dat\"] > bounds[\"cut_ts\"]) & (chunk[\"t_dat\"] <= bounds[\"last_ts\"])\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        lbl = chunk.loc[mask, [\"customer_id\", \"article_id\"]].drop_duplicates()\n",
    "        if lbl.empty:\n",
    "            continue\n",
    "\n",
    "        lbl[\"label\"] = 1\n",
    "        lbl[\"week_end\"] = np.int32(w)\n",
    "\n",
    "        table = pa.Table.from_pandas(\n",
    "            lbl[[\"customer_id\", \"article_id\", \"label\", \"week_end\"]],\n",
    "            preserve_index=False,\n",
    "        )\n",
    "\n",
    "        writer = label_writers.get(w)\n",
    "        if writer is None:\n",
    "            label_writers[w] = pq.ParquetWriter(\n",
    "                label_paths[w],\n",
    "                table.schema,\n",
    "                compression=\"snappy\",\n",
    "            )\n",
    "            writer = label_writers[w]\n",
    "\n",
    "        writer.write_table(table)\n",
    "\n",
    "    del chunk\n",
    "    if idx % 10 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "for writer in label_writers.values():\n",
    "    writer.close()\n",
    "\n",
    "# Deduplicate and finalize labels\n",
    "total_positive = 0\n",
    "for w, path in label_paths.items():\n",
    "    if os.path.exists(path):\n",
    "        week_df = pd.read_parquet(path, engine=\"pyarrow\")\n",
    "        if not week_df.empty:\n",
    "            week_df = week_df.drop_duplicates([\"customer_id\", \"article_id\", \"week_end\"])\n",
    "            week_df[\"label\"] = week_df[\"label\"].astype(\"int8\")\n",
    "        else:\n",
    "            week_df = empty_labels_df()\n",
    "        week_df.to_parquet(path, index=False)\n",
    "        total_positive += len(week_df)\n",
    "        del week_df\n",
    "    else:\n",
    "        empty_labels_df().to_parquet(path, index=False)\n",
    "\n",
    "print(\"Total positive labels (all weeks):\", total_positive)\n",
    "gc.collect()\n",
    "\n",
    "# 2 min, 12 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "078fbced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using feature stream files:\n",
      "Train: ['../data/outputs/features_stream_week=20200819.parquet', '../data/outputs/features_stream_week=20200826.parquet', '../data/outputs/features_stream_week=20200902.parquet', '../data/outputs/features_stream_week=20200909.parquet']\n",
      "Valid: ../data/outputs/features_stream_week=20200916.parquet\n",
      "\n",
      "============================================================\n",
      "Processing week 20200819 (TRAIN)\n",
      "============================================================\n",
      "Training customers: 66199\n",
      "Sampled negatives: 2000000\n",
      "Positives kept:   40551\n",
      "Saved: ../data/outputs/train_part_week=20200819_temp.parquet  rows=2,040,551\n",
      "\n",
      "============================================================\n",
      "Processing week 20200826 (TRAIN)\n",
      "============================================================\n",
      "Training customers: 74010\n",
      "Sampled negatives: 2000000\n",
      "Positives kept:   43777\n",
      "Saved: ../data/outputs/train_part_week=20200826_temp.parquet  rows=2,043,777\n",
      "\n",
      "============================================================\n",
      "Processing week 20200902 (TRAIN)\n",
      "============================================================\n",
      "Training customers: 77684\n",
      "Sampled negatives: 2000000\n",
      "Positives kept:   47317\n",
      "Saved: ../data/outputs/train_part_week=20200902_temp.parquet  rows=2,047,317\n",
      "\n",
      "============================================================\n",
      "Processing week 20200909 (TRAIN)\n",
      "============================================================\n",
      "Training customers: 77091\n",
      "Sampled negatives: 2000000\n",
      "Positives kept:   48437\n",
      "Saved: ../data/outputs/train_part_week=20200909_temp.parquet  rows=2,048,437\n",
      "\n",
      "============================================================\n",
      "Processing week 20200916 (VALID)\n",
      "============================================================\n",
      "Training customers: 68613\n",
      "Saved: ../data/outputs/valid_merged_temp.parquet  rows=253,714,158\n",
      "\n",
      "Done building merged train/valid files.\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# Cell 4.2 – Winner-style GLOBAL negative sampling\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "import os, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "NEG_SAMPLES = 2_000_000\n",
    "FEATURE_CHUNK_ROWS = 5_000_000\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "train_files = [f\"../data/outputs/features_stream_week={w}.parquet\" for w in TRAIN_WEEKS]\n",
    "valid_file  = f\"../data/outputs/features_stream_week={VALID_WEEK}.parquet\"\n",
    "\n",
    "all_week_files = train_files + [valid_file]\n",
    "output_files = {}\n",
    "\n",
    "print(\"Using feature stream files:\")\n",
    "print(\"Train:\", train_files)\n",
    "print(\"Valid:\", valid_file)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# GLOBAL-sampling function\n",
    "# --------------------------------------------------------------\n",
    "def sample_global_negatives(all_neg_chunks, target, rng):\n",
    "    \"\"\"Take list of DataFrames containing negatives → return a 1M global sample.\"\"\"\n",
    "    neg_df = pd.concat(all_neg_chunks, ignore_index=True)\n",
    "    if len(neg_df) <= target:\n",
    "        return neg_df     # small case\n",
    "\n",
    "    return neg_df.sample(target, random_state=rng)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# PROCESS EACH WEEK\n",
    "# --------------------------------------------------------------\n",
    "for f in all_week_files:\n",
    "    week = infer_week_from_features_path(f)\n",
    "    is_valid = (week == VALID_WEEK)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing week {week} ({'VALID' if is_valid else 'TRAIN'})\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Load positive labels\n",
    "    week_labels = load_labels_for_week(week).astype({\n",
    "        \"customer_id\": \"int64\",\n",
    "        \"article_id\":  \"int32\",\n",
    "        \"week_end\":    \"int32\",\n",
    "        \"label\":       \"int8\"\n",
    "    })\n",
    "\n",
    "    # IMPORTANT PART:\n",
    "    # Only customers with positives are used for negative sampling.\n",
    "    training_customers = set(week_labels[\"customer_id\"].unique())\n",
    "    print(f\"Training customers: {len(training_customers)}\")\n",
    "\n",
    "    # Buffers\n",
    "    pos_chunks = []\n",
    "    neg_chunks = []\n",
    "\n",
    "    pqfile = pq.ParquetFile(f)\n",
    "\n",
    "    # ------------ STREAM FEATURES ------------\n",
    "    for batch_idx, batch in enumerate(pqfile.iter_batches(batch_size=FEATURE_CHUNK_ROWS)):\n",
    "\n",
    "        df = batch.to_pandas()\n",
    "\n",
    "        df[\"customer_id\"] = df[\"customer_id\"].astype(\"int64\")\n",
    "        df[\"article_id\"]  = df[\"article_id\"].astype(\"int32\")\n",
    "        df[\"week_end\"]    = df[\"week_end\"].astype(\"int32\")\n",
    "\n",
    "        # Filter to customers with positives → CRITICAL FIX\n",
    "        if not is_valid:\n",
    "            df = df[df[\"customer_id\"].isin(training_customers)]\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # Merge labels\n",
    "        df = df.merge(\n",
    "            week_labels,\n",
    "            on=[\"customer_id\", \"article_id\", \"week_end\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        df[\"label\"] = df[\"label\"].fillna(0).astype(\"int8\")\n",
    "\n",
    "        # Split\n",
    "        pos = df[df.label == 1]\n",
    "        neg = df[df.label == 0]\n",
    "\n",
    "        if not pos.empty:\n",
    "            pos_chunks.append(pos)\n",
    "\n",
    "        if not is_valid and not neg.empty:\n",
    "            neg_chunks.append(neg)\n",
    "        if is_valid and not neg.empty:\n",
    "            neg_chunks.append(neg)   # keep all negs in valid\n",
    "\n",
    "        del df, pos, neg\n",
    "        if batch_idx % 10 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    # ------------- VALID CASE: keep all -------------\n",
    "    if is_valid:\n",
    "        final_df = pd.concat(pos_chunks + neg_chunks, ignore_index=True)\n",
    "\n",
    "    # ------------- TRAIN CASE: global sample 1M negs -------------\n",
    "    else:\n",
    "        pos_df = pd.concat(pos_chunks, ignore_index=True)\n",
    "        neg_df = sample_global_negatives(neg_chunks, NEG_SAMPLES, rng)\n",
    "\n",
    "        final_df = pd.concat([pos_df, neg_df], ignore_index=True)\n",
    "\n",
    "        print(f\"Sampled negatives: {len(neg_df)}\")\n",
    "        print(f\"Positives kept:   {len(pos_df)}\")\n",
    "\n",
    "    # Save\n",
    "    out_tmp = (\n",
    "        \"../data/outputs/valid_merged_temp.parquet\"\n",
    "        if is_valid\n",
    "        else f\"../data/outputs/train_part_week={week}_temp.parquet\"\n",
    "    )\n",
    "\n",
    "    final_df.to_parquet(out_tmp, index=False, compression=\"snappy\")\n",
    "    print(f\"Saved: {out_tmp}  rows={len(final_df):,}\")\n",
    "\n",
    "    # rename train file\n",
    "    if not is_valid:\n",
    "        out_final = out_tmp.replace(\"_temp\", \"\")\n",
    "        os.rename(out_tmp, out_final)\n",
    "\n",
    "    del final_df, pos_chunks, neg_chunks\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\nDone building merged train/valid files.\")\n",
    "\n",
    "# 4m with 2m negatives per week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544380a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing VALID for ranking (PyArrow fast sort)...\n",
      "Sorting Arrow table...\n",
      "Valid rows: 253714158\n",
      "Valid groups: 1371980, avg size=184.93\n",
      "Saved valid_rank.parquet and groups_valid.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 5.1 – Process VALID using PURE PYARROW (Fast, low-memory)\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "valid_rank_path = \"../data/outputs/valid_rank.parquet\"\n",
    "valid_groups_path = \"../data/outputs/groups_valid.npy\"\n",
    "valid_merged_file = \"../data/outputs/valid_merged_temp.parquet\"\n",
    "\n",
    "print(\"Processing VALID for ranking (PyArrow fast sort)...\")\n",
    "\n",
    "# 1. Read file as Arrow table (zero-copy, multithreaded)\n",
    "table = pq.read_table(valid_merged_file)\n",
    "\n",
    "# 2. Build query_id column (vectorized compute)\n",
    "query_id = (\n",
    "    pc.multiply(table[\"week_end\"].cast(pa.int64()),\n",
    "                pa.scalar(100_000_000, pa.int64()))\n",
    ")\n",
    "query_id = pc.add(query_id, table[\"customer_id\"].cast(pa.int64()))\n",
    "table = table.append_column(\"query_id\", query_id)\n",
    "\n",
    "# 3. Add random column for stable shuffle inside each query\n",
    "n = table.num_rows\n",
    "rand_col = pa.array(np.random.RandomState(RANDOM_SEED).rand(n).astype(\"float32\"))\n",
    "table = table.append_column(\"__rand\", rand_col)\n",
    "\n",
    "# 4. Sort by query_id (primary) then random (secondary)\n",
    "print(\"Sorting Arrow table...\")\n",
    "sort_keys = [(\"query_id\", \"ascending\"), (\"__rand\", \"ascending\")]\n",
    "table = table.sort_by(sort_keys)\n",
    "\n",
    "# 5. Compute group sizes (using NumPy view of Arrow buffers)\n",
    "q = table[\"query_id\"].to_numpy(zero_copy_only=False)\n",
    "boundary_idx = np.flatnonzero(np.r_[True, q[1:] != q[:-1], True])\n",
    "valid_group_sizes = np.diff(boundary_idx)\n",
    "\n",
    "# 6. Remove temporary random column\n",
    "table = table.drop_columns([\"__rand\"])\n",
    "\n",
    "# 7. Write final sorted validation file\n",
    "pq.write_table(table, valid_rank_path, compression=\"snappy\")\n",
    "np.save(valid_groups_path, valid_group_sizes)\n",
    "\n",
    "print(f\"Valid rows: {table.num_rows}\")\n",
    "print(f\"Valid groups: {len(valid_group_sizes)}, avg size={table.num_rows/len(valid_group_sizes):.2f}\")\n",
    "print(\"Saved valid_rank.parquet and groups_valid.npy\")\n",
    "\n",
    "# Cleanup\n",
    "del table, q, boundary_idx, valid_group_sizes\n",
    "gc.collect()\n",
    "\n",
    "# 42s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef8dae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing TRAIN for ranking (Global Sort)...\n",
      "Loading ../data/outputs/train_part_week=20200819.parquet...\n",
      "Loading ../data/outputs/train_part_week=20200826.parquet...\n",
      "Loading ../data/outputs/train_part_week=20200902.parquet...\n",
      "Loading ../data/outputs/train_part_week=20200909.parquet...\n",
      "Concatenating...\n",
      "Creating Query IDs...\n",
      "Generating random sort key...\n",
      "Sorting train data (this may take a while)...\n",
      "Calculating groups...\n",
      "Saving train data...\n",
      "Train rows: 8180082\n",
      "Train groups: 294983 (Avg size: 27.73)\n",
      "Saved train_rank.parquet and groups_train.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5.2 – Process TRAIN (Load ALL parts, Sort, Write)\n",
    "train_rank_path = \"../data/outputs/train_rank.parquet\"\n",
    "train_groups_path = \"../data/outputs/groups_train.npy\"\n",
    "train_parts_files = [f\"../data/outputs/train_part_week={w}.parquet\" for w in TRAIN_WEEKS]\n",
    "\n",
    "print(\"\\nProcessing TRAIN for ranking (Global Sort)...\")\n",
    "\n",
    "# 1. Load ALL train parts\n",
    "# OPTIMIZATION: Load one by one and append to list, then concat.\n",
    "dfs = []\n",
    "for f in train_parts_files:\n",
    "    print(f\"Loading {f}...\")\n",
    "    _df = pd.read_parquet(f)\n",
    "    dfs.append(_df)\n",
    "\n",
    "print(\"Concatenating...\")\n",
    "df_train = pd.concat(dfs, ignore_index=True)\n",
    "del dfs\n",
    "gc.collect()\n",
    "\n",
    "# 2. Create Query ID\n",
    "print(\"Creating Query IDs...\")\n",
    "df_train[\"query_id\"] = (\n",
    "    df_train[\"week_end\"].astype(\"int64\") * 100_000_000\n",
    "    + df_train[\"customer_id\"].astype(\"int64\")\n",
    ")\n",
    "\n",
    "# 3. GLOBAL SORT\n",
    "print(\"Generating random sort key...\")\n",
    "rng = np.random.RandomState(RANDOM_SEED)\n",
    "df_train[\"__rand\"] = rng.rand(len(df_train))\n",
    "\n",
    "print(\"Sorting train data (this may take a while)...\")\n",
    "df_train = df_train.sort_values([\"query_id\", \"__rand\"])\n",
    "df_train.drop(columns=[\"__rand\"], inplace=True)\n",
    "\n",
    "# 4. Calculate Groups\n",
    "print(\"Calculating groups...\")\n",
    "q_ids = df_train[\"query_id\"].values\n",
    "unique_indices = np.flatnonzero(np.r_[True, q_ids[1:] != q_ids[:-1], True])\n",
    "train_group_sizes = np.diff(unique_indices)\n",
    "\n",
    "# 5. Save\n",
    "print(\"Saving train data...\")\n",
    "table = pa.Table.from_pandas(df_train, preserve_index=False)\n",
    "pq.write_table(table, train_rank_path, compression=\"snappy\")\n",
    "np.save(train_groups_path, train_group_sizes)\n",
    "\n",
    "print(f\"Train rows: {len(df_train)}\")\n",
    "print(f\"Train groups: {len(train_group_sizes)} (Avg size: {len(df_train)/len(train_group_sizes):.2f})\")\n",
    "print(\"Saved train_rank.parquet and groups_train.npy\")\n",
    "\n",
    "del df_train, table, q_ids, unique_indices\n",
    "gc.collect()\n",
    "\n",
    "# 2 min, 4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a672c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model features: 34\n",
      "Saved ../data/outputs/dataset_meta.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 – choose model features & save dataset_meta.json\n",
    "\n",
    "# We no longer have train_full in memory; infer columns from one train part\n",
    "example_train_path = train_parts_files[0]\n",
    "example_df = pd.read_parquet(example_train_path)\n",
    "\n",
    "drop_cols = [\"label\", \"week_end\", \"query_id\"]\n",
    "id_cols   = [\"customer_id\", \"article_id\"]\n",
    "\n",
    "model_features = [\n",
    "    c for c in example_df.columns\n",
    "    if c not in drop_cols + id_cols\n",
    "]\n",
    "\n",
    "print(\"Number of model features:\", len(model_features))\n",
    "del example_df\n",
    "gc.collect()\n",
    "\n",
    "# Get row counts from saved files\n",
    "train_rank_rows = sum(\n",
    "    pd.read_parquet(p, columns=[\"customer_id\"]).shape[0]\n",
    "    for p in train_parts_files\n",
    ")\n",
    "valid_rank_rows = pd.read_parquet(\"../data/outputs/valid_rank.parquet\", columns=[\"customer_id\"]).shape[0]\n",
    "\n",
    "with open(\"../data/outputs/dataset_meta.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"train_weeks\": TRAIN_WEEKS,\n",
    "        \"valid_week\": VALID_WEEK,\n",
    "        \"label_lookback_days\": LABEL_LOOKBACK_DAYS,\n",
    "        \"neg_sample_clf\": NEG_SAMPLES,\n",
    "        \"model_features\": model_features,\n",
    "        \"rank_train_rows\": int(train_rank_rows),\n",
    "        \"rank_valid_rows\": int(valid_rank_rows),\n",
    "        \"rank_at\": RANK_TOP_K_EVAL,\n",
    "    }, f)\n",
    "\n",
    "print(\"Saved ../data/outputs/dataset_meta.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5720c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aedcade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527fe7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
